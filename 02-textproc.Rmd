# Text Preprocessing {#textproc}

Preparing text data for NLP analysis presents unique challenges. Text can vary significantly in degree and components of “messiness” compared to other common data types depending on where it was sourced from, what types of information is being expressed, and what elements of “noise” as in unwanted components within the text may be present. 

*Text preprocessing* refers to the wide variety of techniques to prepare your text data from its starting point of being loaded into your program to a form ready for whatever methods you’d like to apply to your text. The preprocessing steps you use are therefore highly dependent on both your individual dataset and the specific means of analysis you’re planning on for your text data. Some preprocessing methods are for much more specific use cases, while there’s a general set of recurring techniques often required for NLP applications. 

I’ll also introduce a core theme within NLP in this section that you’ll find throughout this guide in that NLP research findings are often heavily defined by previous choices you make in your research workflow. Text preprocessing decisions are one of the biggest examples of this. How you decide to prepare your data can make a significant impact on the results you obtain further in your research. I will therefore provide an overview of a wide range of preprocessing options and encourage you to test out a variety of these for your specific use cases. This is key to ensure the robustness of your findings to various text specifications and will strengthen the rigor of your research overall. 

This demo focuses on four of my go-to R packages for NLP research that all stem from the *tidyverse* collection by Hadley Wickham and his team at R Studio. These packages are designed to be easy to understand & implement and they seamlessly build from each other for various tasks relevant to your data preparation. **readr** facilitates the easy uploading of external data files such as CSVs into R while **dplyr** promotes streamlined data frame creation and manipulation from said files. **stringr** is all about working with character strings which is the data type that text falls under. **tidytext** provides more specialized functions to prepare text data for various NLP use cases.

I’ll go ahead and load all of these packages into our working session under the assumption of them being previously installed to your local machine through the install.packages() command. I'll also read in our r/nyc dataset from the provided CSV file sourced from this guide's Github repository. I recommend that you create a folder on your computer that holds both the CSV data and a saved R file titled “text_preprocessing.R” in one location.
  
```{r, results=FALSE, message=FALSE}
library(readr)
library(dplyr)
library(stringr)
library(tidytext)

## EDIT THIS PATH FOR FINAL GUIDE LAUNCH
nyc <- read_csv("nyc_reddit.csv")
```

## Character Encoding

*Character encoding* refers to numerical codes designed for programs to know how to display a text character. Particularly messy text sources such as website scrapes can lead to datasets that follow different types of character encoding schemes. This can create issues for reading in and manipulating text within a given program and/or method application. Most English text data is used with either Unicode/UTF-8 or ASCII encoding, and we can check if our dataset of r/nyc comments follows one of these encoding schemas after loading in our data. 

```{r}
guess_encoding(nyc$body)
```
Our 1 confidence score on the UTF-8 encoding for the text column of our data frame gives us assurance that the following preprocessing methods should work without issues around character encoding. Let’s now inspect our dataset in more detail. 

```{r}
nyc 
```

This provides us with the first 10 records of our dataset. “id” is the unique identifier for each individual comment, “author” provides the Reddit username who wrote the comment, “body” features the text of the comment itself, “score” refers to the Reddit system of being able to “upvote” or “downvote” comments similar to a like/dislike framework, and “date” as the year, month, and day the comment was posted. Our preprocessing will focus on “body” as the main text data feature of our data. 

##  String Cleaning

String manipulation is key to virtually any form of cleaning you may need to implement for your data to remove unwanted noise within text and represent a given document in a more interpretable form. This is where functions in the stringr package particularly shine. 

Let’s first consider one semantically meaningless token found throughout our dataset referring to “\n”. This represents line breaks within HTML web syntax that were captured as a by-product of scraping our Reddit comments. We can use stringr’s str_remove_all function to strip out all instances of \n directly from the comment text. 

```{r}
nyc$body <- str_remove_all(nyc$body, "\n")
```

This successfully removes all instances of “\n”. However, there is a variety of additional components of the text that a user may want to extract out depending on their intended NLP use case. For example, removing all punctuation is commonly desired within NLP methods since punctuation would be tokenized as their own words. Our previous method is great at removing any exact match with “\n” within the comments, but a more streamlined technique to remove any undesired character type would save us much more time and code lines then individually having to identify every type of punctuation we'd like to remove.

## Regular Expressions

*Regular expressions*- commonly shorted to regexes- is the core tool within string manipulation to perform more comprehensive removal operations for a variety of use cases. regexes allow you to identify patterns within text and perform operations on them. Regex design can get quite complex and successfully constructing them for certain use cases is another topic in itself. They also must be handled with care, as their potentially intricate design may end up performing manipulation on your text data that you weren’t originally intending to occur. Regexes are however a highly flexible tool for solving various problems within text manipulation. Refer to this chapter’s references at the end of the user guide for further resources regarding the potential designs and applications of regexes as well. 

Given that the remaining noise in our r/nyc comments features a wide variety of punctuation characters types, a regex that identifies and removes all punctuation will serve our purposes particularly well. The special regex we’ll use is [:punct:] which successfully identifies all punctuation characters. Removing said punctuation is achieved through another call with str_remove_all as follows:

```{r}
nyc$body <- str_remove_all(nyc$body, '[:punct:]')
```

## Lowercasing & Concatenation

It’s common within text preprocessing for NLP to convert all words to lowercase since the use of capitalization may lead to a given method to identify the same word as separate entities based on casing alone. You may also consider explicitly combining n-grams into single word units, such as concatenating all instances of the bi-gram “New York” into the single token of “New_York”. This isn’t as commonly employed within NLP as lowercasing is, but there’s certain use cases where having n-grams explicitly connected to each other can be key to your research interests. You’ll find stringr-based techniques for lowercasing and concatenating respectively as follows:

```{r}
nyc$body <- str_to_lower(nyc$body)
nyc$body <- str_replace_all(nyc$body, "New York", "New_York")
```

## Tokenization

*Tokenization* as mentioned in this guide’s first chapter refers to the segmentation of text components into a given unit of analysis such as individual words, sentences, paragraphs, characters, or n-grams. While the best type of tokenization is highly dependent on your given research questions and planned analyses, the most common tokenization unit overall are individual words. I therefore employ tidytext’s unnest_tokens method to create a separate data frame with a designated row for each individual word within a r/nyc comment. The “word” argument specifies individual words as our tokenization unit. 

```{r}
tokens <- nyc %>%
    unnest_tokens("word", body)
```

Our 100,000 original comments are now almost 3.5 million rows of individual word tokens. By default, unnest_tokens lowercases the tokens and removes punctuation from the original string, allowing you to skip stringr’s to_lower and having to use a punctuation removal regex if you’re also planning on tokenizing your text data through tidytext. 

## Optional Method-Dependent Preprocessing Steps 

There’s a variety of additional preprocessing techniques you may consider for your specific NLP research interests beyond the previously delineated methods. These include stopword removal, stemming, and parts-of-speech tagging. These techniques are overall more case-by-base in their usefulness than string manipulation and tokenization are as core preprocessing steps. Even methods where techniques such as stopword removal or lemmatization are common are often a source of controversy within the NLP community around whether said removal is appropriate. It’s often best to see if your results change either with or without a given preprocessing technique used before your analysis, and if they do consider said differences in the interpretation of your overall research findings. 

### Stopwords

*Stopwords* is the term form common words such as “the”, “but”, and “is” that are often understood as contextually irrelevant for the primary purposes of a given NLP analysis. Stop word removal can be particularly helpful when you’d like to focus on unique and/or subject-specific words that often carry more in-depth meaning than more common phrases. Topic models are one form of NLP analysis we’ll be exploring later in this guide where stop word removal is quite common, as the method is usually employed to discover conceptually deeper themes within text than what semantically generic words tend to indicate. However, other use cases such as exploring how two communities differ in conversational styles may be better served by keeping stop words, since even the usage of common words can indicate meaningful differences between groups. 

If stop word removal seems relevant to your given research question, tidytext actually provides an established dataset of common English stopwords for you to remove from your tokenized text. This is achieved through an *anti-join* of your own data and the stopwords data frame, which builds from query-language data merging principles to not include words that match both datasets. It’s also important to note that this preestablished set of stopwords may not entirely translate to other common and conceptually less meaningful words within your own data. It’s therefore common to build from an initial list and perform additional stopword removal through techniques such as the previously delineated str_remove_all. 

```{r, results=FALSE, message=FALSE}
data(stop_words)

tokens <- tokens %>%
  anti_join(stop_words)
```

### Stemming

*Stemming* refers to reducing words to their base forms. This technique is often used to group together different words that have the same underlying root such as “swims”, “swimmer”, and “swimming” all referring to the base stem of “swim”. Stemming can be helpful to streamline vocabulary size, such as for machine learning NLP techniques where having less words that are similar to each other can cut down on processing time and necessary computational resources for running already complex models. However, said similar words via a core base root can also have very different conceptual meanings and be used in distinct contexts from each other within text that stemming may minimize or lead to a misinterpretation of. This is a technique that therefore needs to be used with particular care towards weighing both its potential costs and benefits. 

There’s a variety of different types of stemming algorithms available with the most commonly used being the Porter stemmer. You’ll have to download the SnowballC package separately to use the Porter stemming technique. I demonstrate its application on our previously created tidytext word token data frame below:

```{r}
library(SnowballC)

stems <- tokens %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)
```

### Parts-of-Speech

Part-of-speech (POS) tagging describes the process of identifying the lexical and grammatical category a word belongs to within a sentence, such as nouns, adverbs, conjunctions, and beyond. POS tags are often used for analyses where the type of words within text is of particular importance to your research interests. Potential cases of this include how actors interact with each other as expressed in a given sentence’s structure or how the frequency of certain linguistic features within text may be predictive of an outcome. Annotating each token’s respective POS type can be used to generate counts of overall categories or discover the most common words that fall into a given lexical class within your specific text. 

One disadvantage of POS tagging is that it can take a long time to process an entire tokenized corpus with each word’s matching POS class. Different libraries that support POS tagging often require you to download and load a separate annotation model that maps words to their correct POS categories as well. The udpipe package streamlines this process with its concurrent library commands delineated in the following code for our tokenized r/nyc comments below:

```{r, eval=FALSE}
library("udpipe")

model <- udpipe_download_model(language = "english")
model <- udpipe_load_model(model)

## Annotation will take quite some time, particularly if you're using the 
## non-stopword version of our tokenized dataset 
tags <- udpipe_annotate(model, x = tokens$word)

## The generated "upos" column in the following dataframe will have each token's 
## identified POS tags.
pos <- as.data.frame(tags)
```
