# Machine Learning & Topic Models {#ml}

*Machine learning* (ML) has been rapidly increasing in popularity within academia as data science and computational approaches have become more widely known across disciplines. The term can be intimidating for those unfamiliar with its theory and methods. A simplified conceptualization of machine learning is the application of algorithms- a set of instructions grounded in foundational math and statistics- to identify relationships within data to produce informed predictions on. The "learning" in machine learning refers to these method’s ability to perform additional steps within its analysis building from directly programmed instructions via the relationships it is able to learn within your data set. These methods therefore excel at categorization, pattern recognition, and identifying large-scale trends through how each model learns the data it is trained on. 

## Supervised vs. Unsupervised Learning

Machine learning methods are often differentiated between supervised and unsupervised approaches. Supervised methods are based on labeled data sets where a model learns a relationship to produce a predicted outcome value. "Labeled" data refers to the identified information about the data regarding the features and characteristics of an individual record directly provided to the ML model. Said specified features must be identified ahead of time to use supervised learning methods as the model uses these previously delineated examples to become "trained" in predicting an outcome of interest towards data with similar features. The developed model is then able to replicate its learned task to previously unobserved and often unlabeled data regarding the original factors it’s trained to identify. 

In contrast, unsupervised approaches let the model guide the learning process through found relationships that were previously unmarked within the data source. Unsupervised methods are particularly well-suited for exploratory research where relationships between factors may not be already known to a given researcher. This is why unsupervised ML methods for NLP are often of distinct interest to social scientists. These tools intrinsically guide emergent findings within data sources and are well suited for large amounts of nuanced text where full-corpus relationships and trends may not be readily known.

While machine learning is often associated with Python programming, it’s a lot easier to implement ML models that many social scientists realize within R. There are more and more outstanding R libraries that reduce the learning curve complexity of many ML methods and aggregate a variety of relevant ML packages into a centralized framework. 

## Topic Models 

The method we’ll be highlighting in this demo is topic modeling. Topic models refers to a class of unsupervised ML algorithms that identify common themes within a text corpus by assigning an associated collection of words to a set number of topics. The method is very effective at identifying underlying trends regarding what is commonly discussed within a given text corpus. Topic probabilities are first set to random and then updated as more documents are reviewed by the model. The model processes how words co-occur with each other across the documents of the corpus to create its topical clusters. 

While topic models are powered by unsupervised machine learning, their discovery of themes within text doesn’t account for the inherent meaning of the identified topics themselves. Interpreting topic model results is a great opportunity for social scientists to employ their domain knowledge to decode how the words assigned to each topic relate to each other. 

## Structural Topic Models

There are a variety of topic modeling methods with the most commonly employed being Latent Dirichlet Allocation (LDA). We’ll be using an extension of LDA known as Structural Topic Models (STM) to analyze our r/nyc data that was developed by the political scientists and sociologists Molly Roberts, Brandon Stewart, and Dustin Tingley. STM allows for text documents to be associated with multiple identified topics. While I’ll keep the technical details of how STM works brief for the purposes of this introduction, you can think through the model’s underlying techniques as driven by the goal of generating the best topics with their associated words that maximizes the likelihood of having produced the actual observed word trends within your text data. 

The innovation that STM offers over LDA is linking relevant metadata within your dataset to their variation across topics as well. This essentially incorporates a “covariate” design similar to regression models that are already familiar to quantitative social scientists. We’ll therefore investigate how topic prevalence varies with with a comment’s associated score in our r/nyc sample. 

### Preprocessing

Compared to the dictionary-based sentiment analysis in the previous demo which can handle text data often in its original raw format, topic models are a method that requires extensive pre-processing. Word tokenization, punctuation removal, lowercasing, and stopword removal are all standard steps to prepare data for topic models. For the start of our demo, I first load in our required library and datasets. I then use STM’s built-in document preprocessing function which performs many of the same cleaning and preparation steps featured in the text preprocessing chapter of this guide under the hood. The textProcessor() function’s first argument is the data’s text column, while the metadata parameters refers to any of the remaining covariate you may be interested in exploring their variance with the generated topics. 

```{r, message=FALSE, warning = FALSE, results = FALSE}
library(readr)
library(stm)

nyc <- read_csv("nyc_reddit.csv")

processed <- textProcessor(nyc$body, metadata = nyc, stem = FALSE)
```

Having finished our necessary preprocessing of the r/nyc comments, we then move to reshaping the text into a specific format for topic models known as the *document-term matrix*. Within this matrix each word token is a row, and each column represents a document within the corpus. We also generate separate objects of the entire vocabulary across our documents as well as for the potential metadata covariate features. All of these components need to be separately prepared to build our topic model’s necessary parameters for analysis. 

```{r, message=FALSE, warning = FALSE, results = FALSE}
out <- prepDocuments(processed$documents, processed$vocab, processed$meta) 
docs <- out$documents
vocab <- out$vocab 
meta <- out$meta
```

### Fitting the Model

We’re now ready to fit our model. I’ve arbitrarily decided to set our “k” parameter- referring to the number of topics- as 15. This is a comparatively small number for the size of our dataset and the sheer variety of topics that are likely being discussed on r/nyc, but I’ll keep our model small for the sake of our introductory demonstration. I highly recommend Roberts, Stewart, and Tingley’s [provided package vignette]( https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) for STM regarding some features included within the library that can aid with choosing an optimal number of topics with more empirically robust metrics. 

STM is a complex model that can take some time to run. I’d say it’s generally faster than the VADER analyzer used within the previous guide as a reference for readers who are following along with the chapters in sequence. If you’d like to skip this processing time I included a “nyc_STM.RData” environment in the Github that is available for download. Loading in this environment will automatically have the finished STM results available within your R session, and you’ll be able to skip right to the summary() call of the model results. 

```{r, eval=FALSE}
nyc_redditFit <- stm(documents = out$documents, vocab = out$vocab,
                                              K = 15, prevalence =~ score, data = out$meta,
                                              init.type = "Spectral")
```

```{r, echo=FALSE}
load("~/Documents/GitHub/nlp-for-socsci/nyc_STM.RData")
```

```{r}
summary(nyc_redditFit)
```

Our 15 generated topics seem to cover a representative range of themes within r/nyc. STM produces a range of associated words with each topic based on different metrics, but we’ll just focus on the first line within each topic that lists the words with the highest probability of being associated with said topic when they occur in a comment. 

Topic 1 seems to be about local housing, topic 2 about work, topic 3 with words around user engagement in r/nyc related to reading posts and links, topic 4 as informal response words within comment conversation, and so forth. This demonstrates some key takeaways around topic models. First, subjective interpretation is key to understanding what it is the model output means when grouping associated words. Second, some topics demonstrate much more concise associations regarding their underlying subject over others. 

Let’s deep dive into associated text with topic 15 which has rather conceptually ambiguous associated terms to understand what type of discourse the model is identifying as linked to this topic. The findThoughts() function allows us to retrieve three r/nyc comments that are aligned with topic 15. 

```{r}
thoughts15 <- findThoughts(nyc_redditFit, texts = out$meta$body, topics = 15, n=3)
thoughts15
```

These three comments suggests that Topic 15 is picking up discourse around user judgements of appropriate behavior and actions, particularly within argumentative discussions where it appears there are disagreements occurring within the comment conversation thread. This is an example of just how essential close text readings often are within NLP analysis to link model findings to conceptual meanings regarding complex linguistic phenomena. 

### Variation by Comment Score

Let’s do a final investigation within our topic model by allowing topic prevalence to vary by a r/nyc comment’s associated score. Calling estimateEffect() with the “formula” argument specifying which covariate you’d like to vary by topic will achieve this. You can use the summary() command to get the score parameter association with each topic similar to what we obtained with previously summarizing the initial model. I went ahead and highlighted two topics in particular to minimize our net output.  
```{r}
score_topics <- estimateEffect(formula = 1:15 ~ score, nyc_redditFit, meta=out$meta, uncertainty="Global")

summary(score_topics, topics=c(11, 14))
```

Topic 11’s most probable words includes “problem”, “business”, and “restaurants”, while topic 14 is associated with “water”, “food”, “vote”, and “mayor”. We can see from estimateEffect’s results that topic 11 is significantly associated with comments that receive lower score values, while topic 14 is linked with comments that obtain higher scores. Investigating the associated comments would be a natural next step from these findings to see if additional trends emerge within the text itself that may be relevant to how the comment is reacted to within the wider r/nyc community.  

### Extending Machine Learning

STM is an inherently probabilistic model and will therefore often produce different results depending on your choices of the number of topics and different random number seeds. Topics can be hard to interpret and you’ll want to watch out for issues around multi-meaning “chimera” topics or semantically meaningless topics. Choosing the proper number of topics is rather unsettled process and often significant varies within research applications. Despite these considerations, topic models can be one of the best options for exploratory analysis of text data, particularly for discovering underlying themes and patterns that may not be already known to the researcher.

Topic models are just one of many machine learning methods supporting NLP research within R. Additional modeling approaches are a whole separate subtopic beyond this introductory guide with a [variety](https://smltar.com/) of [similar](https://bradleyboehmke.github.io/HOML/) [resources](https://www.manning.com/books/deep-learning-with-r) that I highly recommend. If there’s a core takeaway I’d like to emphasize with this demo, it’s that ML is a lot more accessible to implement within both R specifically and for previously unfamiliar researchers in general than many realize. 
