<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Machine Learning &amp; Topic Models | Natural Language Processing for the Social Sciences</title>
  <meta name="description" content="Chapter 4 Machine Learning &amp; Topic Models | Natural Language Processing for the Social Sciences User Guide." />
  <meta name="generator" content="bookdown 0.24.3 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Machine Learning &amp; Topic Models | Natural Language Processing for the Social Sciences" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://ccss-rs/nlp-for-socsci" />
  
  <meta property="og:description" content="Chapter 4 Machine Learning &amp; Topic Models | Natural Language Processing for the Social Sciences User Guide." />
  <meta name="github-repo" content="ccss-rs/nlp-for-socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Machine Learning &amp; Topic Models | Natural Language Processing for the Social Sciences" />
  
  <meta name="twitter:description" content="Chapter 4 Machine Learning &amp; Topic Models | Natural Language Processing for the Social Sciences User Guide." />
  

<meta name="author" content="Remy Stewart" />


<meta name="date" content="2021-12-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dictionary.html"/>
<link rel="next" href="methodsethics.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Fundamental Concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#text-as-data"><i class="fa fa-check"></i><b>1.1</b> Text as Data</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#natural-language-processing"><i class="fa fa-check"></i><b>1.2</b> Natural Language Processing</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#computational-social-sciences"><i class="fa fa-check"></i><b>1.3</b> Computational Social Sciences</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#nlp-core-vocabulary"><i class="fa fa-check"></i><b>1.4</b> NLP Core Vocabulary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="textproc.html"><a href="textproc.html"><i class="fa fa-check"></i><b>2</b> Text Preprocessing</a>
<ul>
<li class="chapter" data-level="2.1" data-path="textproc.html"><a href="textproc.html#character-encoding"><i class="fa fa-check"></i><b>2.1</b> Character Encoding</a></li>
<li class="chapter" data-level="2.2" data-path="textproc.html"><a href="textproc.html#string-cleaning"><i class="fa fa-check"></i><b>2.2</b> String Cleaning</a></li>
<li class="chapter" data-level="2.3" data-path="textproc.html"><a href="textproc.html#regular-expressions"><i class="fa fa-check"></i><b>2.3</b> Regular Expressions</a></li>
<li class="chapter" data-level="2.4" data-path="textproc.html"><a href="textproc.html#lowercasing-concatenation"><i class="fa fa-check"></i><b>2.4</b> Lowercasing &amp; Concatenation</a></li>
<li class="chapter" data-level="2.5" data-path="textproc.html"><a href="textproc.html#tokenization"><i class="fa fa-check"></i><b>2.5</b> Tokenization</a></li>
<li class="chapter" data-level="2.6" data-path="textproc.html"><a href="textproc.html#optional-method-dependent-preprocessing-steps"><i class="fa fa-check"></i><b>2.6</b> Optional Method-Dependent Preprocessing Steps</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="textproc.html"><a href="textproc.html#stopwords"><i class="fa fa-check"></i><b>2.6.1</b> Stopwords</a></li>
<li class="chapter" data-level="2.6.2" data-path="textproc.html"><a href="textproc.html#stemming"><i class="fa fa-check"></i><b>2.6.2</b> Stemming</a></li>
<li class="chapter" data-level="2.6.3" data-path="textproc.html"><a href="textproc.html#parts-of-speech"><i class="fa fa-check"></i><b>2.6.3</b> Parts-of-Speech</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dictionary.html"><a href="dictionary.html"><i class="fa fa-check"></i><b>3</b> Dictionaries &amp; Sentiment Analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dictionary.html"><a href="dictionary.html#types-of-dictionaries"><i class="fa fa-check"></i><b>3.1</b> Types of Dictionaries</a></li>
<li class="chapter" data-level="3.2" data-path="dictionary.html"><a href="dictionary.html#sentiment-analysis"><i class="fa fa-check"></i><b>3.2</b> Sentiment Analysis</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="dictionary.html"><a href="dictionary.html#vader"><i class="fa fa-check"></i><b>3.2.1</b> VADER</a></li>
<li class="chapter" data-level="3.2.2" data-path="dictionary.html"><a href="dictionary.html#string-sentiment"><i class="fa fa-check"></i><b>3.2.2</b> String Sentiment</a></li>
<li class="chapter" data-level="3.2.3" data-path="dictionary.html"><a href="dictionary.html#rnyc-sentiment"><i class="fa fa-check"></i><b>3.2.3</b> r/nyc Sentiment</a></li>
<li class="chapter" data-level="3.2.4" data-path="dictionary.html"><a href="dictionary.html#sentiment-score"><i class="fa fa-check"></i><b>3.2.4</b> Sentiment &amp; Score</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="dictionary.html"><a href="dictionary.html#beyond-dictionaries"><i class="fa fa-check"></i><b>3.3</b> Beyond Dictionaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i><b>4</b> Machine Learning &amp; Topic Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ml.html"><a href="ml.html#supervised-vs.-unsupervised-learning"><i class="fa fa-check"></i><b>4.1</b> Supervised vs. Unsupervised Learning</a></li>
<li class="chapter" data-level="4.2" data-path="ml.html"><a href="ml.html#topic-models"><i class="fa fa-check"></i><b>4.2</b> Topic Models</a></li>
<li class="chapter" data-level="4.3" data-path="ml.html"><a href="ml.html#structural-topic-models"><i class="fa fa-check"></i><b>4.3</b> Structural Topic Models</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ml.html"><a href="ml.html#preprocessing"><i class="fa fa-check"></i><b>4.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="4.3.2" data-path="ml.html"><a href="ml.html#fitting-the-model"><i class="fa fa-check"></i><b>4.3.2</b> Fitting the Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="ml.html"><a href="ml.html#variation-by-comment-score"><i class="fa fa-check"></i><b>4.3.3</b> Variation by Comment Score</a></li>
<li class="chapter" data-level="4.3.4" data-path="ml.html"><a href="ml.html#extending-machine-learning"><i class="fa fa-check"></i><b>4.3.4</b> Extending Machine Learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="methodsethics.html"><a href="methodsethics.html"><i class="fa fa-check"></i><b>5</b> Further Applications &amp; Ethical Considerations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="methodsethics.html"><a href="methodsethics.html#embeddings"><i class="fa fa-check"></i><b>5.1</b> Embeddings</a></li>
<li class="chapter" data-level="5.2" data-path="methodsethics.html"><a href="methodsethics.html#supervised-classification"><i class="fa fa-check"></i><b>5.2</b> Supervised Classification</a></li>
<li class="chapter" data-level="5.3" data-path="methodsethics.html"><a href="methodsethics.html#networks"><i class="fa fa-check"></i><b>5.3</b> Networks</a></li>
<li class="chapter" data-level="5.4" data-path="methodsethics.html"><a href="methodsethics.html#causality"><i class="fa fa-check"></i><b>5.4</b> Causality</a></li>
<li class="chapter" data-level="5.5" data-path="methodsethics.html"><a href="methodsethics.html#ethics-of-nlp"><i class="fa fa-check"></i><b>5.5</b> Ethics of NLP</a></li>
<li class="chapter" data-level="5.6" data-path="methodsethics.html"><a href="methodsethics.html#future-directions"><i class="fa fa-check"></i><b>5.6</b> Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a>
<ul>
<li class="chapter" data-level="6.1" data-path="references.html"><a href="references.html#chapter-1--introduction"><i class="fa fa-check"></i><b>6.1</b> Chapter 1- Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="references.html"><a href="references.html#data-and-the-information-age"><i class="fa fa-check"></i><b>6.1.1</b> Data and the Information Age</a></li>
<li class="chapter" data-level="6.1.2" data-path="references.html"><a href="references.html#css-and-nlp"><i class="fa fa-check"></i><b>6.1.2</b> CSS and NLP</a></li>
<li class="chapter" data-level="6.1.3" data-path="references.html"><a href="references.html#nlp-resources-specific-to-r-applications"><i class="fa fa-check"></i><b>6.1.3</b> NLP Resources Specific to R Applications</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="references.html"><a href="references.html#chapter-2--text-preprocessing"><i class="fa fa-check"></i><b>6.2</b> Chapter 2- Text Preprocessing</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="references.html"><a href="references.html#text-preprocessing"><i class="fa fa-check"></i><b>6.2.1</b> Text Preprocessing</a></li>
<li class="chapter" data-level="6.2.2" data-path="references.html"><a href="references.html#stemming-stopwords-and-parts-of-speech"><i class="fa fa-check"></i><b>6.2.2</b> Stemming, Stopwords, and Parts of Speech</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="references.html"><a href="references.html#chapter-3-dictionaries-sentiment-analysis"><i class="fa fa-check"></i><b>6.3</b> Chapter 3 – Dictionaries &amp; Sentiment Analysis</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="references.html"><a href="references.html#counts-dictionary-based-studies"><i class="fa fa-check"></i><b>6.3.1</b> Counts &amp; Dictionary Based Studies</a></li>
<li class="chapter" data-level="6.3.2" data-path="references.html"><a href="references.html#liwc"><i class="fa fa-check"></i><b>6.3.2</b> LIWC</a></li>
<li class="chapter" data-level="6.3.3" data-path="references.html"><a href="references.html#sentiment-analysis-1"><i class="fa fa-check"></i><b>6.3.3</b> Sentiment Analysis</a></li>
<li class="chapter" data-level="6.3.4" data-path="references.html"><a href="references.html#vader-1"><i class="fa fa-check"></i><b>6.3.4</b> VADER</a></li>
<li class="chapter" data-level="6.3.5" data-path="references.html"><a href="references.html#alternative-sentiment-analysis-approaches"><i class="fa fa-check"></i><b>6.3.5</b> Alternative Sentiment Analysis Approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="references.html"><a href="references.html#chapter-4--machine-learning-topic-models"><i class="fa fa-check"></i><b>6.4</b> Chapter 4- Machine Learning &amp; Topic Models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="references.html"><a href="references.html#machine-learning-for-social-science-annual-reviews"><i class="fa fa-check"></i><b>6.4.1</b> Machine Learning for Social Science Annual Reviews</a></li>
<li class="chapter" data-level="6.4.2" data-path="references.html"><a href="references.html#topic-model-papers"><i class="fa fa-check"></i><b>6.4.2</b> Topic Model Papers</a></li>
<li class="chapter" data-level="6.4.3" data-path="references.html"><a href="references.html#structural-topic-models-author-papers"><i class="fa fa-check"></i><b>6.4.3</b> Structural Topic Models Author Papers</a></li>
<li class="chapter" data-level="6.4.4" data-path="references.html"><a href="references.html#css-research-using-topic-models"><i class="fa fa-check"></i><b>6.4.4</b> CSS Research Using Topic Models</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="references.html"><a href="references.html#chapter-5--further-applications-ethics"><i class="fa fa-check"></i><b>6.5</b> Chapter 5- Further Applications &amp; Ethics</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="references.html"><a href="references.html#further-applications"><i class="fa fa-check"></i><b>6.5.1</b> Further Applications</a></li>
<li class="chapter" data-level="6.5.2" data-path="references.html"><a href="references.html#ethics-within-nlp"><i class="fa fa-check"></i><b>6.5.2</b> Ethics within NLP</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Natural Language Processing for the Social Sciences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ml" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Machine Learning &amp; Topic Models</h1>
<p><em>Machine learning</em> (ML) has been rapidly increasing in popularity within academia as data science and computational approaches have become more widely known across disciplines. The term can be intimidating for those unfamiliar with its theory and methods. A simplified conceptualization of machine learning is the application of algorithms- a set of instructions grounded in foundational math and statistics- to identify relationships within data to produce informed predictions on. The “learning” in machine learning refers to these method’s ability to perform additional steps within its analysis building from directly programmed instructions via the relationships it is able to learn within your data set. These methods therefore excel at categorization, pattern recognition, and identifying large-scale trends through how each model learns the data it is trained on.</p>
<div id="supervised-vs.-unsupervised-learning" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Supervised vs. Unsupervised Learning</h2>
<p>Machine learning methods are often differentiated between supervised and unsupervised approaches. Supervised methods are based on labeled data sets where a model learns a relationship to produce a predicted outcome value. “Labeled” data refers to data that has identified features and characteristics that are directly provided to the ML model. Said specified features must be identified ahead of time to use supervised learning methods as the model uses these previously delineated examples to become “trained” in predicting an outcome of interest towards data with similar features. The developed model is then able to replicate its learned task to previously unobserved and often unlabeled data regarding the original factors it’s trained to identify.</p>
<p>In contrast, unsupervised approaches let the model guide the learning process through found relationships that were previously unmarked within the data source. Unsupervised methods are well-suited for exploratory research where relationships between factors may not be already known to a given researcher. This is why unsupervised ML methods for NLP are often of distinct interest to social scientists. These tools guide emergent findings within text data and are well suited for large amounts of nuanced text where full-corpus relationships and trends may not be readily known.</p>
<p>While machine learning is often associated with Python programming, it’s a lot easier to implement ML models that many social scientists realize within R. There are more and more outstanding R libraries that reduce the learning curve complexity of many ML methods and aggregate a variety of relevant ML packages into a centralized framework.</p>
</div>
<div id="topic-models" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Topic Models</h2>
<p>The method we’ll be highlighting in this demo is topic modeling. Topic models refers to a class of unsupervised ML algorithms that identify common themes within a text corpus by assigning an associated collection of words to a set number of topics. The method is very effective at identifying underlying trends regarding what is commonly discussed within a corpus. Topic probabilities are first set to random and then updated as more documents are reviewed by the model. The model processes how words co-occur with each other across the documents of the corpus to create its topical clusters.</p>
<p>While topic models are powered by unsupervised machine learning, their discovery of themes within text doesn’t account for the inherent meaning of the identified topics themselves. Interpreting topic model results is a great opportunity for social scientists to employ their domain knowledge to decode how the words assigned to each topic relate to each other.</p>
</div>
<div id="structural-topic-models" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Structural Topic Models</h2>
<p>There are a variety of topic modeling methods with the most commonly employed being Latent Dirichlet Allocation (LDA). We’ll be using an extension of LDA known as Structural Topic Models (STM) to analyze our r/nyc data that was developed by the political scientists and sociologists Molly Roberts, Brandon Stewart, and Dustin Tingley. STM allows for text documents to be associated with multiple identified topics. While I’ll keep the details of STM’s design brief for the purposes of this introduction, you can think through the model’s underlying techniques as driven by the goal of generating the best topics with their associated words that appear to be the most likely to have produced the actual observed word trends within your text data.</p>
<p>The innovation that STM offers over LDA is linking relevant metadata within your data set to their variation across topics as well. This essentially incorporates a “covariate” design similar to regression models that are already familiar to quantitative social scientists. We’ll therefore investigate how topic prevalence varies with with a comment’s associated score in our r/nyc sample.</p>
<div id="preprocessing" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Preprocessing</h3>
<p>Compared to the dictionary-based sentiment analysis in the previous demo which can handle text data often in its original raw format, topic models are a method that requires extensive pre-processing. Word tokenization, punctuation removal, lowercasing, and stopword removal are all standard steps to prepare data for topic models. For the start of our demo, I first load in our required library and datasets. I then use STM’s built-in document preprocessing function which performs many of the same cleaning and preparation steps featured in the text preprocessing chapter of this guide. The textProcessor() function’s first argument is the data’s text column, while the metadata parameters refers to any covariates you may be interested in exploring their association across generated topics with.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="ml.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb28-2"><a href="ml.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stm)</span>
<span id="cb28-3"><a href="ml.html#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="ml.html#cb28-4" aria-hidden="true" tabindex="-1"></a>nyc <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;nyc_reddit.csv&quot;</span>)</span>
<span id="cb28-5"><a href="ml.html#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="ml.html#cb28-6" aria-hidden="true" tabindex="-1"></a>processed <span class="ot">&lt;-</span> <span class="fu">textProcessor</span>(nyc<span class="sc">$</span>body, <span class="at">metadata =</span> nyc, <span class="at">stem =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Having finished our necessary preprocessing of the r/nyc comments, we then move to reshaping the text into a specific format for topic models known as the <em>document-term matrix</em>. Within this matrix each word token is a row, and each column represents a document within the corpus. We also generate separate objects of the entire vocabulary across our documents as well as for the potential metadata covariate features. All of these components need to be separately prepared to build our topic model’s necessary parameters for analysis.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="ml.html#cb29-1" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">prepDocuments</span>(processed<span class="sc">$</span>documents, processed<span class="sc">$</span>vocab, processed<span class="sc">$</span>meta) </span>
<span id="cb29-2"><a href="ml.html#cb29-2" aria-hidden="true" tabindex="-1"></a>docs <span class="ot">&lt;-</span> out<span class="sc">$</span>documents</span>
<span id="cb29-3"><a href="ml.html#cb29-3" aria-hidden="true" tabindex="-1"></a>vocab <span class="ot">&lt;-</span> out<span class="sc">$</span>vocab </span>
<span id="cb29-4"><a href="ml.html#cb29-4" aria-hidden="true" tabindex="-1"></a>meta <span class="ot">&lt;-</span> out<span class="sc">$</span>meta</span></code></pre></div>
</div>
<div id="fitting-the-model" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Fitting the Model</h3>
<p>We’re now ready to fit our model. I’ve arbitrarily decided to set our “k” parameter- referring to the number of topics- as 15. This is a comparatively small number for the size of our dataset and the sheer variety of topics that are likely being discussed on r/nyc, but I’ll keep our model small for the sake of our introductory demonstration. I highly recommend Roberts, Stewart, and Tingley’s <a href="https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf">provided package vignette</a> for STM regarding some features included within the library that can aid with choosing an optimal number of topics with more empirically robust metrics.</p>
<p>STM is a complex model that can take some time to run. I’d say it’s generally faster than the VADER analyzer used within the previous guide as a reference for readers who are following along with the chapters in sequence. If you’d like to skip this processing time I included a “nyc_STM.RData” environment in the Github that is available for download. Loading in this environment will automatically have the finished STM results available within your R session, and you’ll be able to skip right to the summary() call of the model results.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="ml.html#cb30-1" aria-hidden="true" tabindex="-1"></a>nyc_redditFit <span class="ot">&lt;-</span> <span class="fu">stm</span>(<span class="at">documents =</span> out<span class="sc">$</span>documents, <span class="at">vocab =</span> out<span class="sc">$</span>vocab,</span>
<span id="cb30-2"><a href="ml.html#cb30-2" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">K =</span> <span class="dv">15</span>, <span class="at">prevalence =</span><span class="sc">~</span> score, <span class="at">data =</span> out<span class="sc">$</span>meta,</span>
<span id="cb30-3"><a href="ml.html#cb30-3" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">init.type =</span> <span class="st">&quot;Spectral&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="ml.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(nyc_redditFit)</span></code></pre></div>
<pre><code>## A topic model with 15 topics, 99345 documents and a 30552 word dictionary.</code></pre>
<pre><code>## Topic 1 Top Words:
##       Highest Prob: money, pay, will, building, housing, rent, cost 
##       FREX: rent, units, tenants, tax, nuclear, landlords, tenant 
##       Lift: carbon, condos, -gw, ‘affordable’, “affordable, “affordable”, “deal” 
##       Score: money, housing, rent, pay, tax, cost, building 
## Topic 2 Top Words:
##       Highest Prob: work, job, kids, school, year, home, office 
##       FREX: remote, unemployment, pension, teachers, ems, teacher, unions 
##       Lift: -tipping, clerical, cnas, entry-level, gnt, grayson, homeschool 
##       Score: work, school, kids, job, workers, office, teachers 
## Topic 3 Top Words:
##       Highest Prob: post, show, read, can, use, article, find 
##       FREX: card, info, excelsior, link, cards, database, wallet 
##       Lift: “excelsior, “record, acct, airpods, asknyc, authenticate, authenticated 
##       Score: card, post, article, read, thanks, thank, app 
## Topic 4 Top Words:
##       Highest Prob: like, just, one, time, now, good, really 
##       FREX: yeah, damn, ive, gonna, hope, wish, seen 
##       Lift: “excuse, actin, backpacks, battlefield, beaters, bedsheets, buisness 
##       Score: like, just, got, lol, yeah, guy, one 
## Topic 5 Top Words:
##       Highest Prob: don’t, ’re, think, people, say, just, point 
##       FREX: racist, downvoted, blm, ’re, slur, don’t, troll 
##       Lift: “flaw”, “q”, apostrophe, assholes”, backpedal, context”, eloquently 
##       Score: don’t, ’re, racist, comment, doesn’t, think, can’t 
## Topic 6 Top Words:
##       Highest Prob: shit, fuck, fucking, little, long, island, big 
##       FREX: rats, holy, staten, baby, sounds, cool, omg 
##       Lift: conman, nadlers, perfluorocarbon, raccoon, stroganoff, rats, -ppch 
##       Score: fuck, shit, island, fucking, staten, sounds, cool 
## Topic 7 Top Words:
##       Highest Prob: state, government, right, law, case, power, hes 
##       FREX: constitution, constitutional, amendment, religious, religion, supreme, church 
##       Lift: amendment, “fascist, “give, “group”, “human, aborted, abortion” 
##       Score: government, state, law, religious, hes, court, rights 
## Topic 8 Top Words:
##       Highest Prob: vaccine, covid, vaccinated, people, get, vaccines, getting 
##       FREX: vaccines, virus, immunity, delta, flu, disease, infection 
##       Lift: -preparedness, ade, anti-scientific, antigen, antiviral, apcs, arising 
##       Score: vaccine, vaccinated, covid, vaccines, immunity, virus, unvaccinated 
## Topic 9 Top Words:
##       Highest Prob: new, day, last, york, thought, first, two 
##       FREX: bacon, spotted, twin, tower, cheese, bec, empire 
##       Lift: -acre, -piece, -story, -unit, ‘first-class, “’d, “deeply 
##       Score: new, york, saw, night, thought, last, day 
## Topic 10 Top Words:
##       Highest Prob: nyc, city, live, many, people, world, much 
##       FREX: whites, communities, capita, asian, rural, latino, asians 
##       Lift: “racial, achievers, actionable, anti-authority, binghamton, brown”, burlington 
##       Score: nyc, crime, city, cities, population, areas, live 
## Topic 11 Top Words:
##       Highest Prob: problem, business, restaurant, restaurants, private, etc, order 
##       FREX: lyft, medallions, medallion, restaurants, customers, dining, cabs 
##       Lift: bode, brokered, carb, caretaker, creditors, defrauded, exclusivity 
##       Score: uber, restaurants, business, delivery, private, problem, restaurant 
## Topic 12 Top Words:
##       Highest Prob: police, cops, talking, nypd, even, yet, guys 
##       FREX: arrest, arrests, robbery, arrested, rape, cops, attempted 
##       Lift: advisable, amerigo, arrest, baptists, boogeymen, bowel, chabad 
##       Score: cops, police, nypd, white, crime, talking, cop 
## Topic 13 Top Words:
##       Highest Prob: car, cars, street, subway, manhattan, train, park 
##       FREX: bike, bus, lane, lanes, bikes, buses, pedestrians 
##       Lift: alleyways, bdfm, bicycles, bidirectional, bikers, bmws, brt 
##       Score: cars, car, bike, parking, train, manhattan, traffic 
## Topic 14 Top Words:
##       Highest Prob: water, food, vote, mayor, trump, run, party 
##       FREX: adams, voting, republican, election, voted, sliwa, voters 
##       Lift: --city, “food, “vote, aberration, aoc’s, aspirations, avella 
##       Score: vote, adams, water, mayor, trump, republican, party 
## Topic 15 Top Words:
##       Highest Prob: people, dont, can, get, want, will, thats 
##       FREX: dont, cant, want, thats, theyre, happen, wont 
##       Lift: happend, bac, frightening, omicron, perjury, reintegrated, ticketable 
##       Score: dont, people, want, thats, youre, cant, get</code></pre>
<p>Our 15 generated topics seem to cover a representative range of themes within r/nyc. STM produces a range of associated words with each topic based on different metrics, but we’ll just focus on the first line within each topic that lists the words with the highest probability of being associated with said topic when they occur in a comment.</p>
<p>Topic 1 seems to be about local housing, topic 2 about work, topic 3 with words around user engagement in r/nyc related to reading posts and links, topic 4 as informal response words within comment conversation, and so forth. This demonstrates some core takeaways as regarding topic models. First, subjective interpretation is key to understanding what it is the model output means when grouping associated words. Second, some topics demonstrate much more concise associations regarding their underlying subject over others.</p>
<p>Let’s investigate the documents that are associated with topic 15 which comprises of rather ambiguous terms to understand what type of conversation the model is identifying. The findThoughts() function allows us to retrieve three r/nyc comments that are aligned with topic 15.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="ml.html#cb34-1" aria-hidden="true" tabindex="-1"></a>thoughts15 <span class="ot">&lt;-</span> <span class="fu">findThoughts</span>(nyc_redditFit, <span class="at">texts =</span> out<span class="sc">$</span>meta<span class="sc">$</span>body, <span class="at">topics =</span> <span class="dv">15</span>, <span class="at">n=</span><span class="dv">3</span>)</span>
<span id="cb34-2"><a href="ml.html#cb34-2" aria-hidden="true" tabindex="-1"></a>thoughts15</span></code></pre></div>
<pre><code>## 
##  Topic 15: 
##       Don&#39;t care! Don&#39;t care if they lose their job! Dont care if they&#39;re mad! Dont care about them because they don&#39;t care about other people!!!!!!
##      So it&#39;s very telling that in a society and a sub where people will constantly say that if you want something done you should do it yourself.....suddenly that&#39;s not a good idea when this man decides to do it.
## 
## Hmmmmm now what could be different.....i can&#39;t QWHITE put my finger on it.
##      So basically all the more reason these people should be considered trash? Keep in mind that&#39;s their decision they&#39;re making to put people at risks bc they&#39;re unwilling to fulfill the obligation of the work they have chosen. It&#39;s easy to satisfy your obligations when it&#39;s easy it&#39;s a lot harder when you have to actually do the hard things, like stuff you don&#39;t *want* to do but will bc you have to.</code></pre>
<p>These three comments suggests that Topic 15 is picking up discourse around user critiques, perhaps occurring within more discourse-oriented discussions. This is an example of just how essential close text readings often are within NLP analysis to link model findings to conceptual meanings regarding complex linguistic phenomena.</p>
</div>
<div id="variation-by-comment-score" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Variation by Comment Score</h3>
<p>Let’s do a final investigation with our topic model by allowing topic prevalence to vary by a r/nyc comment’s associated score. Calling estimateEffect() with the “formula” argument specifying which covariate you’d like to vary by topic will achieve this. You can use the summary() command to get the score parameter associated with each topic similar to what we obtained with previously summarizing the initial model. I went ahead and highlighted two particular topics to minimize our net output.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="ml.html#cb36-1" aria-hidden="true" tabindex="-1"></a>score_topics <span class="ot">&lt;-</span> <span class="fu">estimateEffect</span>(<span class="at">formula =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span> <span class="sc">~</span> score, nyc_redditFit, <span class="at">meta=</span>out<span class="sc">$</span>meta, <span class="at">uncertainty=</span><span class="st">&quot;Global&quot;</span>)</span>
<span id="cb36-2"><a href="ml.html#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="ml.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(score_topics, <span class="at">topics=</span><span class="fu">c</span>(<span class="dv">11</span>, <span class="dv">14</span>))</span></code></pre></div>
<pre><code>## 
## Call:
## estimateEffect(formula = 1:15 ~ score, stmobj = nyc_redditFit, 
##     metadata = out$meta, uncertainty = &quot;Global&quot;)
## 
## 
## Topic 11:
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.038e-02  2.463e-04 163.917   &lt;2e-16 ***
## score       -4.704e-06  7.987e-06  -0.589    0.556    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## 
## Topic 14:
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 4.545e-02  3.267e-04 139.110  &lt; 2e-16 ***
## score       4.934e-05  1.003e-05   4.919  8.7e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Topic 11’s most probable words includes “problem,” “business,” and “restaurants,” while topic 14 is associated with “water,” “food,” “vote,” and “mayor.” We can see from estimateEffect’s results that topic 11 is significantly associated with comments that receive lower score values, while topic 14 is linked with comments that obtain higher scores. Investigating the associated comments would be a natural next step from these findings to see if additional trends emerge within the text itself that may be relevant to how the comment is reacted to within the wider r/nyc community.</p>
</div>
<div id="extending-machine-learning" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Extending Machine Learning</h3>
<p>STM is an inherently probabilistic model and will therefore often produce different results depending on your choices of the number of topics. Topics can be hard to interpret and you’ll want to watch out for issues around multi-meaning “chimera” topics or semantically meaningless topics. Choosing the proper number of topics is rather unsettled process and often significant varies within research applications. Despite these considerations, topic models can be one of the best options for exploratory analysis of text data, particularly for discovering underlying themes and patterns that may not be already known to the researcher.</p>
<p>Topic models are just one of many machine learning methods supporting NLP research within R. Additional modeling approaches are a whole separate subject to explore beyond this introductory guide with a <a href="https://smltar.com/">variety</a> of <a href="https://bradleyboehmke.github.io/HOML/">similar</a> <a href="https://www.manning.com/books/deep-learning-with-r">resources</a> that I highly recommend. If there’s a core takeaway I’d like to emphasize with this demo, it’s that ML is a lot more accessible to implement within both R specifically and for previously unfamiliar researchers in general than many realize.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dictionary.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methodsethics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-ml.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
