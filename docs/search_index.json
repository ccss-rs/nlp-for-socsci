[["index.html", "Natural Language Processing for the Social Sciences Preface", " Natural Language Processing for the Social Sciences Remy Stewart 2021-12-01 Preface Welcome to the Natural Language Processing for Social Sciences User Guide provided by the Cornell Center for Social Sciences (CCSS)! This guide serves as both the written version of Spring 2022’s workshop under the same name as well as a standing reference available to wider Cornell community. Are you… Not too familiar with NLP and are looking for a broad view on the discipline as applicable to social sciences? Curious about ways you could potentially incorporate NLP into your research? Know enough R to understand its basic syntax? In search of simple and interpretable R code to get you started on using NLP? If any of these are true for you, I think you’ll be able to get something out of this guide. This guide is heavily inspired by a whole series of similar works designed to introduce NLP to social scientists or for providing code demos in R for various NLP tasks. Almost all of these resources go further into depth on the topics introduced within this guide and I highly recommend them all as additional references. A little about the author- my name is Remy Stewart and I am both a PhD Candidate in the Sociology Department here at Cornell as well as a Data Science Consultant for CCSS. I use NLP extensively within my own research that investigates how technology impacts public policy and perpetuates inequality within US urban regions. This work therefore unavoidably reflects my own disciplinary training biases, but I intentionally designed it to be as widely applicable to readers across social science fields as possible. You’ll be able to navigate through the individual chapters of the guide through the index to the left. The chapters do build from each other particularly regarding the core concepts and coding components, but can stand alone for each topic as well. "],["intro.html", "Chapter 1 Fundamental Concepts 1.1 Text as Data 1.2 Natural Language Processing 1.3 Computational Social Sciences 1.4 NLP Core Vocabulary", " Chapter 1 Fundamental Concepts 1.1 Text as Data Text is a seminal communication medium within and produced by human interactions and social systems and has been a longstanding data source in social science research. However, text data has often been associated with qualitative scholarship given limitations of predominant quantitative methods for handling text and language. While the numerical analysis of text has been of ongoing interest of fields such as linguistics and information retrieval for decades, the employment of computational text analysis methods across a variety of disciplines has been a comparatively recent phenomenon. Said diffusion of the quantitative study of text is intimately tied to the Digital Revolution’s concurrent facilitation of the accessibility of text data as well as ability to model said records within computationally-powered analyses. Digital traces or footprints refer to the records individuals leave behind of their behavior, thoughts, relationships, and beyond that are digitally recorded and often acquirable by third parties (Golder and Macy 2014). Text data is one of the most common types of digital traces as facilitated by the sharp rise of internet use, particularly as regarding social media platforms. Consider the following trend as visualized by Our World in Data of just how ubiquitous social media has become globally- While some of these platforms focus more on image and video content over text than others, all of them have at least some degree of text produced that is often publicly accessible. Plenty of additional websites such as online marketplaces or business and government sites also produce ample amounts of text to be used within social science research. Beyond internet mediums on their own is also the concurrent shift of preexisting print documents to digitized forms as well as the production of more recent documents to now be available online. Newspaper archives, government records, and books across languages are all examples of these sources. Researchers can now analyze historical documents through new methodological approaches through their digitized versions as well as connect published material across time from decades-old records to modern day. From 1996 to 2021. Source: New York Times. 1.2 Natural Language Processing The rise of the internet and its ubiquitous cataloging of produced text is concurrent to the growth of resources and methods to computationally explore said written records. Natural language processing- shortened to NLP- is the field that specializes in using computational tools to interpret human language. NLP itself refers to a broad range of applications including speech and handwriting recognition, text generation, translation, and beyond. NLP methods range in complexity from simple counts of word occurrences to the ability to generate accurately predicted words through machine learning-powered artificial intelligence. Recent advances in NLP have increasingly been able to engage with more complex questions regarding text, such as the context words occur in, their conceptual similarities to other words, and the potentially multiple meanings of a given word. NLP and its respective methods can often be computationally resource intensive for storing, processing, and interpreting text. These barriers limited its more widespread adoption within research across fields such as the social sciences, along with barriers regarding learning new programming languages and computational skills. Advances in the availability of computing resources have significantly speed up what may have otherwise be painfully slow algorithmic analyses and have made storage systems to save massive collections of text data widely available. The open-source software development movement has additionally lead to innovations in programming resources to employ NLP that are free, methodologically rigorous, and often more accessibly designed to learn. 1.3 Computational Social Sciences The subsequent recent adoption of NLP methods within social science research is a subset of the wider disciplinary establishment of computational social sciences (CSS). Lazer et al. defines CCS as the “development and application of computational methods to complex, typically large-scale, human (sometimes simulated) behavioral data” (2009: 721). CCS spans a wide range of topics beyond text data analysis via NLP with additional core areas including online community network analysis, geospatial tracking, agent-based modeling, and many more examples. However, CSS is particularly well-aligned with NLP methods when considering how much information on human behavior is embedded within text. A particular strength social scientists offer when conducting CSS research with NLP methods is their personal expertise towards interpreting often nuanced and contextualized findings within text data. Social scientists have therefore been on the cutting edge for conducting NLP-powered research across diverse topics and have formed related communities within universities and conferences connecting CSS and NLP practitioners. Word embeddings from Kozlowski, Taddy, and Evans 2019 &amp; Structural Topic Models from Wilkerson &amp; Casas 2017 1.4 NLP Core Vocabulary This guide will offer an introductory demo of how to apply NLP within your own research via R. Before moving to the guided applications, we must review a series of common terms within NLP that this guide will refer to. I’ll ground said terminology by first reviewing the dataset that all of the following chapters will use and is available on the guide’s GitHub repository. Our dataset is 100,000 comments scraped from “r/nyc” which is the largest community on the social media site Reddit for posts and conversation around anything related to New York City. I obtained these records through a recent call at the time of writing to the Pushshift Application Programming Interface (API) created by Jason Baumgartner. Reddit is known as the “Front Page of the Internet” and it is one of the most common social media sites studied within NLP research. Features of the platform that contribute to Reddit’s popularity within NLP is that its data is accessible to acquire, it features a range of sub-communities of potential interest, and it is a particularly discussion-orientated platform. I also note that using Reddit data- as is the case with virtually all publicly accessible online trace data- comes with its own ethical considerations regarding privacy, sensitivity, and expectations around use within research. Please refer to the “Ethics” section in the final chapter of this guide for further thoughts around said topics. Landing page of r/nyc. Now that we’ve familiarized ourselves with our dataset, let’s consider how we would refer to its respective components through NLP terminology. A corpus refers to the total collection of text being analyzed. This would be all 100k of our r/nyc Reddit comments. A vocabulary is the entire collection of unique words that occur within a given corpus. A document is an individual record within the corpus. This would be one particular collected r/nyc comment and all of its associated text. An example would be “My favorite pizza place in Hell’s Kitchen is 99 Cent Fresh Pizza. I really love their BBQ Chicken Slice.” Tokens are the smaller composite units within a given document. “Tokens” on its own most commonly refers to individual words, but the actual process of tokenization can feature larger units such as sentences or paragraphs. When referring to the tokens of the document example above, “My,” “favorite,” “pizza,” and “place” would be considered as four separate tokens of the 21 total tokens including the sentence’s periods. However, another NLP method may instead operationalize this comment’s tokens as “My favorite pizza place in Hell’s Kitchen is 99 Cent Fresh Pizza.” and “I really love their BBQ Chicken Slice.” Tokenization can also be conducted to character-level granularity, such as “p,” “i,” “z,” “z,” “a”… and following through all other words within the comment. N-grams considers multiple word tokens at once where “n” refers to a specified number. They are often employed for representing words that are conceptually different when considered as a whole unit rather than as individual tokens. The bi-gram of “Hell’s_Kitchen” is understood as a specific neighborhood through each word’s co-occurrence with each other than what “Hell’s” or “Kitchen” would imply on their own. These are the core terms necessary for an initial engagement with NLP methods and are often referred to directly within the R software packages we will be using in the following demos. With our core understanding of NLP and our dataset now cemented, let’s move on to our first key step within our text data analysis starting with text preprocessing in the next chapter. "],["textproc.html", "Chapter 2 Text Preprocessing 2.1 Character Encoding 2.2 String Cleaning 2.3 Regular Expressions 2.4 Lowercasing &amp; Concatenation 2.5 Tokenization 2.6 Optional Method-Dependent Preprocessing Steps", " Chapter 2 Text Preprocessing Preparing text data for NLP analysis presents unique challenges. Text can vary significantly in degree and components of “messiness” compared to other common data types depending on where it was sourced from, what types of information is being expressed, and what elements of “noise” as in unwanted components within the text may be present. Text preprocessing refers to the wide variety of techniques to prepare your text data from its starting point of being loaded into your program to a form ready for whatever methods you’d like to apply to your text. The preprocessing steps you use are therefore highly dependent on both your individual dataset and the specific means of analysis you’re planning on for your text data. Some preprocessing methods are for much more specific use cases, while there’s a general set of recurring techniques often required for NLP applications. I’ll also introduce a core theme within NLP in this section that you’ll find throughout this guide in that NLP research findings are often heavily defined by previous choices you make in your research workflow. Text preprocessing decisions are one of the biggest examples of this. How you decide to prepare your data can make a significant impact on the results you obtain further in your research. I will therefore provide an overview of a wide range of preprocessing options and encourage you to test out a variety of these for your specific use cases. This is key to ensure the robustness of your findings to various text specifications and will strengthen the rigor of your research overall. This demo focuses on four of my go-to R packages for NLP research that all stem from the tidyverse collection by Hadley Wickham and his team at R Studio. These packages are designed to be easy to understand &amp; implement and they seamlessly build from each other for various tasks relevant to your data preparation. readr facilitates the easy uploading of external data files such as CSVs into R while dplyr promotes streamlined data frame creation and manipulation from said files. stringr is all about working with character strings which is the data type that text falls under. tidytext provides more specialized functions to prepare text data for various NLP use cases. I’ll go ahead and load all of these packages into our working session under the assumption of them being previously installed to your local machine through the install.packages() command. I’ll also read in our r/nyc dataset from the provided CSV file sourced from this guide’s Github repository. I recommend that you create a folder on your computer that holds both the CSV data and a saved R file titled “text_preprocessing.R” in one location. library(readr) library(dplyr) library(stringr) library(tidytext) ## EDIT THIS PATH FOR FINAL GUIDE LAUNCH nyc &lt;- read_csv(&quot;nyc_reddit.csv&quot;) 2.1 Character Encoding Character encoding refers to numerical codes designed for programs to know how to display a text character. Particularly messy text sources such as website scrapes can lead to datasets that follow different types of character encoding schemes. This can create issues for reading in and manipulating text within a given program and/or method application. Most English text data is used with either Unicode/UTF-8 or ASCII encoding, and we can check if our dataset of r/nyc comments follows one of these encoding schemas after loading in our data. guess_encoding(nyc$body) ## # A tibble: 2 × 2 ## encoding confidence ## &lt;chr&gt; &lt;dbl&gt; ## 1 UTF-8 1 ## 2 windows-1252 0.68 Our 1 confidence score on the UTF-8 encoding for the text column of our data frame gives us assurance that the following preprocessing methods should work without issues around character encoding. Let’s now inspect our dataset in more detail. nyc ## # A tibble: 100,000 × 5 ## id author body score date ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2666 chillwavexyx &quot;Not just my experience, unfortunately. Very hard to take them on their word after what they… 1 11/4/21 ## 2 30225 MG7444 &quot;Was Joe Biden disciplined? AOC? Any of the Democrats who are routinely photographed and vid… -10 10/21/… ## 3 85724 robswins &quot;Imagine if cities actually used police like this. There are dozens of places I see where pe… 27 9/20/21 ## 4 13927 nydutch &quot;No, it really, really is not. I don&#39;t see any hoookers pissing in the middle of 42nd st.&quot; 1 10/30/… ## 5 117584 kittygirl9891 &quot;I&#39;m so worried about my car. It&#39;s parked in a basement garage. 😭&quot; 7 9/2/21 ## 6 100790 Darrkman &quot;Oh lord. \\n\\nI bet you&#39;re 30 yrs old at the oldest. \\n\\nDude I&#39;m not gonna explain to you a… 1 9/10/21 ## 7 69805 Solagnas &quot;You inferred wrong, idk what to tell you man. Its efficacy is what it is, it&#39;s not more or … -2 9/28/21 ## 8 138396 TheNormalAlternative &quot;Last I checked, this bridge was 30 miles north of NYC.&quot; -6 8/20/21 ## 9 125491 couchTomatoe &quot;Yes, I said that. Every corner.&quot; 2 8/27/21 ## 10 96017 twelvydubs &quot;Earlier in the year we already extended WFH to early 2022. Now it&#39;s been extended even deep… 19 9/13/21 ## # … with 99,990 more rows This provides us with the first 10 records of our dataset. “id” is the unique identifier for each individual comment, “author” provides the Reddit username who wrote the comment, “body” features the text of the comment itself, “score” refers to the Reddit system of being able to “upvote” or “downvote” comments similar to a like/dislike framework, and “date” as the year, month, and day the comment was posted. Our preprocessing will focus on “body” as the main text data feature of our data. 2.2 String Cleaning String manipulation is key to virtually any form of cleaning you may need to implement for your data to remove unwanted noise within text and represent a given document in a more interpretable form. This is where functions in the stringr package particularly shine. Let’s first consider one semantically meaningless token found throughout our dataset referring to “” This represents line breaks within HTML web syntax that were captured as a by-product of scraping our Reddit comments. We can use stringr’s str_remove_all function to strip out all instances of directly from the comment text. nyc$body &lt;- str_remove_all(nyc$body, &quot;\\n&quot;) This successfully removes all instances of “” However, there is a variety of additional components of the text that a user may want to extract out depending on their intended NLP use case. For example, removing all punctuation is commonly desired within NLP methods since punctuation would be tokenized as their own words. Our previous method is great at removing any exact match with “” within the comments, but a more streamlined technique to remove any undesired character type would save us much more time and code lines then individually having to identify every type of punctuation we’d like to remove. 2.3 Regular Expressions Regular expressions- commonly shorted to regexes- is the core tool within string manipulation to perform more comprehensive removal operations for a variety of use cases. regexes allow you to identify patterns within text and perform operations on them. Regex design can get quite complex and successfully constructing them for certain use cases is another topic in itself. They also must be handled with care, as their potentially intricate design may end up performing manipulation on your text data that you weren’t originally intending to occur. Regexes are however a highly flexible tool for solving various problems within text manipulation. Refer to this chapter’s references at the end of the user guide for further resources regarding the potential designs and applications of regexes as well. Given that the remaining noise in our r/nyc comments features a wide variety of punctuation characters types, a regex that identifies and removes all punctuation will serve our purposes particularly well. The special regex we’ll use is [:punct:] which successfully identifies all punctuation characters. Removing said punctuation is achieved through another call with str_remove_all as follows: nyc$body &lt;- str_remove_all(nyc$body, &#39;[:punct:]&#39;) 2.4 Lowercasing &amp; Concatenation It’s common within text preprocessing for NLP to convert all words to lowercase since the use of capitalization may lead to a given method to identify the same word as separate entities based on casing alone. You may also consider explicitly combining n-grams into single word units, such as concatenating all instances of the bi-gram “New York” into the single token of “New_York.” This isn’t as commonly employed within NLP as lowercasing is, but there’s certain use cases where having n-grams explicitly connected to each other can be key to your research interests. You’ll find stringr-based techniques for lowercasing and concatenating respectively as follows: nyc$body &lt;- str_to_lower(nyc$body) nyc$body &lt;- str_replace_all(nyc$body, &quot;New York&quot;, &quot;New_York&quot;) 2.5 Tokenization Tokenization as mentioned in this guide’s first chapter refers to the segmentation of text components into a given unit of analysis such as individual words, sentences, paragraphs, characters, or n-grams. While the best type of tokenization is highly dependent on your given research questions and planned analyses, the most common tokenization unit overall are individual words. I therefore employ tidytext’s unnest_tokens method to create a separate data frame with a designated row for each individual word within a r/nyc comment. The “word” argument specifies individual words as our tokenization unit. tokens &lt;- nyc %&gt;% unnest_tokens(&quot;word&quot;, body) Our 100,000 original comments are now almost 3.5 million rows of individual word tokens. By default, unnest_tokens lowercases the tokens and removes punctuation from the original string, allowing you to skip stringr’s to_lower and having to use a punctuation removal regex if you’re also planning on tokenizing your text data through tidytext. 2.6 Optional Method-Dependent Preprocessing Steps There’s a variety of additional preprocessing techniques you may consider for your specific NLP research interests beyond the previously delineated methods. These include stopword removal, stemming, and parts-of-speech tagging. These techniques are overall more case-by-base in their usefulness than string manipulation and tokenization are as core preprocessing steps. Even methods where techniques such as stopword removal or lemmatization are common are often a source of controversy within the NLP community around whether said removal is appropriate. It’s often best to see if your results change either with or without a given preprocessing technique used before your analysis, and if they do consider said differences in the interpretation of your overall research findings. 2.6.1 Stopwords Stopwords is the term form common words such as “the,” “but,” and “is” that are often understood as contextually irrelevant for the primary purposes of a given NLP analysis. Stop word removal can be particularly helpful when you’d like to focus on unique and/or subject-specific words that often carry more in-depth meaning than more common phrases. Topic models are one form of NLP analysis we’ll be exploring later in this guide where stop word removal is quite common, as the method is usually employed to discover conceptually deeper themes within text than what semantically generic words tend to indicate. However, other use cases such as exploring how two communities differ in conversational styles may be better served by keeping stop words, since even the usage of common words can indicate meaningful differences between groups. If stop word removal seems relevant to your given research question, tidytext actually provides an established dataset of common English stopwords for you to remove from your tokenized text. This is achieved through an anti-join of your own data and the stopwords data frame, which builds from query-language data merging principles to not include words that match both datasets. It’s also important to note that this preestablished set of stopwords may not entirely translate to other common and conceptually less meaningful words within your own data. It’s therefore common to build from an initial list and perform additional stopword removal through techniques such as the previously delineated str_remove_all. data(stop_words) tokens &lt;- tokens %&gt;% anti_join(stop_words) 2.6.2 Stemming Stemming refers to reducing words to their base forms. This technique is often used to group together different words that have the same underlying root such as “swims,” “swimmer,” and “swimming” all referring to the base stem of “swim.” Stemming can be helpful to streamline vocabulary size, such as for machine learning NLP techniques where having less words that are similar to each other can cut down on processing time and necessary computational resources for running already complex models. However, said similar words via a core base root can also have very different conceptual meanings and be used in distinct contexts from each other within text that stemming may minimize or lead to a misinterpretation of. This is a technique that therefore needs to be used with particular care towards weighing both its potential costs and benefits. There’s a variety of different types of stemming algorithms available with the most commonly used being the Porter stemmer. You’ll have to download the SnowballC package separately to use the Porter stemming technique. I demonstrate its application on our previously created tidytext word token data frame below: library(SnowballC) stems &lt;- tokens %&gt;% mutate(stem = wordStem(word)) %&gt;% count(stem, sort = TRUE) 2.6.3 Parts-of-Speech Part-of-speech (POS) tagging describes the process of identifying the lexical and grammatical category a word belongs to within a sentence, such as nouns, adverbs, conjunctions, and beyond. POS tags are often used for analyses where the type of words within text is of particular importance to your research interests. Potential cases of this include how actors interact with each other as expressed in a given sentence’s structure or how the frequency of certain linguistic features within text may be predictive of an outcome. Annotating each token’s respective POS type can be used to generate counts of overall categories or discover the most common words that fall into a given lexical class within your specific text. One disadvantage of POS tagging is that it can take a long time to process an entire tokenized corpus with each word’s matching POS class. Different libraries that support POS tagging often require you to download and load a separate annotation model that maps words to their correct POS categories as well. The udpipe package streamlines this process with its concurrent library commands delineated in the following code for our tokenized r/nyc comments below: library(&quot;udpipe&quot;) model &lt;- udpipe_download_model(language = &quot;english&quot;) model &lt;- udpipe_load_model(model) ## Annotation will take quite some time, particularly if you&#39;re using the ## non-stopword version of our tokenized dataset tags &lt;- udpipe_annotate(model, x = tokens$word) ## The generated &quot;upos&quot; column in the following dataframe will have each token&#39;s ## identified POS tags. pos &lt;- as.data.frame(tags) "],["dictionary.html", "Chapter 3 Dictionaries &amp; Sentiment Analysis 3.1 Types of Dictionaries 3.2 Sentiment Analysis 3.3 Beyond Dictionaries", " Chapter 3 Dictionaries &amp; Sentiment Analysis Counting the prevalence and types of words used is a fundamental component of NLP that more complex methods often build from. Word counts offer plentiful information on what it being discussed within a given corpus, how common or rare said words are, who tends to use certain words, and how they differ in their use from other groups of interests. This chapter will provide both an overview of how word counts power the domain of dictionary-based NLP methods as well as walk through an application of counting words that fall into types of emotional sentiment within our r/nyc dataset. Baseline word counts often feature either the entire vocabulary or the counts for all words within a given document. Of particular interest for many social scientists is the prevalence of words within text as relevant to an abstract concept or theme. This is where lexical dictionaries shine, referring to any established collection of words used to measure a certain phenomenon that can be applied to various use cases. The measurement of said concept– while scaled and scored in different forms – is often based on initial word counts. Dictionary-based methods use designated lexicons designed to accurately capture a variety of words that all measure the concept of interest. Some dictionary methods focus on general lexical features such as pronouns, grammar usage, or asked questions. Other dictionaries attempt to measure more abstract ideas such as identifying words that indicate power, politeness, toxicity, and beyond. For example, a politeness dictionary would have a predetermined list of relevant words such as “please” and “thanks” that is then implemented to measure the count of these words within a text dataset. Dictionaries require the user to predetermine the relationship between a word and a concept. They can be applied across domains to varying degrees depending on the words that comprise the dictionary, the rigor in which the dictionary was created, and the ideas it attempts to measure. 3.1 Types of Dictionaries Dictionaries within text-based social science research are divided between those that are built by the researcher for their specific project, and those that were developed by other scholars and then shared for wider use. Building one’s own dictionary is particularly useful when conducting research relevant to specific communities with unique word usage patterns or when attempting to operationalize concepts with otherwise limited precedent within text methods. Robust dictionary design for specialized applications is a unique skill that many social scientists can offer, since we are often domain experts in niche social phenomenon and therefore have particular knowledge towards what words should be included. However, creating one’s own dictionary also has limitations, such as requiring more resources and labor to rigorously develop than sourcing already established dictionaries. The challenges associated with developing one’s own dictionary is a sizable factor behind the popularity of using pre-established dictionaries within research. The creation of a large and conceptually rigorous dictionary is often a project that a whole team has spent considerable time on to identify candidate words from valid sources, cross-check term applicability through relevant agreement metrics, conduct experiments on the dictionary’s validity towards measuring its concept of interest, and finally review its potential applications across other research projects. One of the most famous dictionaries used within social science research is the Linguistic Inquiry Word Count (LIWC). LIWC includes over 70 categories across a variety of topics regarding both linguistic features and psychological metrics with a rigorous word validation process backing each dictionary. Its ability to measure complex phenomenon within text such as emotions, motivational drives, and cognitive processes has driven its use within a wide variety of projects across social science disciplines. A selection of available dictionaries within LIWC LIWC is a great resource that is both rigorously created and well established within the literature. However, it is a proprietary product which limits dictionary access both cost-wise and regarding keeping the individual words within each dictionary private. This can make it challenging to investigate how well said dictionaries will translate over to your specific text data. Part of the beauty of NLP’s interdisciplinary growth is the availability of open-source dictionaries. There are a wide range of options, and I highly recommend conducting reviews on any specific topic you may be interested in exploring within your text data to see what dictionaries may align with your interests. Much like with making your own dictionary, however, you’ll want to bear in mind the rigor and domain applicability of pre-made dictionaries. It’s often key to review whatever details are available regarding how the dictionary is made to understand its empirical robustness and relevance to your specific use case. This may be provided through resources such as publications that explain the process behind a dictionary’s creation or documentation within a dictionary’s repository or website when accessing its public files. 3.2 Sentiment Analysis Sentiment analysis as applied to product reviews. A NLP method that has been adopted across disciplines and is commonly based on established dictionaries is sentiment analysis. Most sentiment classification systems identify individual words as expressing positive, negative, or neutral sentiments and subsequently generates scores of the overall sentiment within a text. Sentiment analysis is often used to understand emotions around significant social events, consumer preferences, and political opinions. While a variety of computational methods can be employed to measure sentiment, one of the most common types of measurement tools are analyzers comprised of multiple dictionaries with words coded as expressing either of the three sentiments. There’s a range of dictionary-based sentiment analyzers that have been developed and tested on various sources such as tweets, online market product reviews, newspaper opinion sections, or movie reviews. This diversity of sentiment tools subsequently can differ in their dictionary terms and how they compute their sentiment scores. Intended sentiment within text is often highly contextual. Sentiment analysis methods therefore also differ in their consideration of intention within word usage. Intention with the negative sentiment of “cheap” referring to a bad product review takes a different tone when considering a “cheap” deal within an online marketplace. Applying a generic analysis tool in a context where a particular negative or positive phrase is used disproportionately often will likely produce inaccurate results. For example, “loss” as an often negatively categorized word within a sentiment dictionary would likely misrepresent many posts within a weight loss discussion forum. The influence of context is therefore essential to consider when choosing a sentiment dictionary for a specific research application. 3.2.1 VADER The Valence Aware Dictionary and Sentiment Reasoner- shortened to VADER- is the dictionary-based sentiment analyzer that we’ll be using in this demo. It was originally written in Python but is available in R through the VADER library. The analyzer’s sentiment dictionaries were created by a team of human raters that scored candidate words for both their emotional polarity (such as positive over negative) and intensity (“amazing” having a higher score than “okay”). VADER generates four scores when applied to text data- positive, negative, neutral, and a compound score that considers all three of these scores to provide an aggregate representation of a text’s overall sentiment. VADER is often the sentiment analyzer of choice for studies that use Reddit data since it is designed to account for common lexical features within social media text. VADER considers more complex influences towards intended sentiment when generating its scores such as negation with “I don’t like,” the underlying sentiment of emoticons and emojis, and the use of emphasis, capitalization, and punctuation. It is therefore a particularly well-suited tool for investigating our r/nyc dataset in the following demo. We’ll first load in our relevant dictionaries and dataset. You’ll want to reload the r/nyc data even if you’re following along from the previous text preprocessing chapter to ensure your text data is consistent with the demo as follows. Please install any new packages for your local computer as listed below via install.packages() as well. library(readr) library(dplyr) library(stringr) library(vader) library(ggplot2) nyc &lt;- read_csv(&quot;nyc_reddit.csv&quot;) 3.2.2 String Sentiment Let’s consider examples of individual strings to hone our intuition regarding how VADER generates its sentiment scores. The get_vader() function is our tool of choice to produce individual string scores which I apply to three examples as follows. get_vader(&quot;As someone who always depended on cars before, I LOVE the subway! &lt;3&quot;) ## word_scores compound ## &quot;{0, 0, 0, 0, 0, 0, 0, 0, 0, 3.933, 0, 0, 1.9}&quot; &quot;0.845&quot; ## pos neu ## &quot;0.425&quot; &quot;0.575&quot; ## neg but_count ## &quot;0&quot; &quot;0&quot; get_vader(&quot;The subway is very helpful, but I&#39;m not a fan of the rats.&quot;) ## word_scores compound ## &quot;{0, 0, 0, 0, 1.0465, 0, 0, 0, 0, -1.443, 0, 0, 0}&quot; &quot;-0.102&quot; ## pos neu ## &quot;0.132&quot; &quot;0.71&quot; ## neg but_count ## &quot;0.158&quot; &quot;1&quot; get_vader(&quot;I hate how delayed the subway always is… being late for work sucks. :(&quot;) ## word_scores compound ## &quot;{0, -2.7, 0, -0.9, 0, 0, 0, 0, 0, 0, 0, 0, -1.5, -1.9}&quot; &quot;-0.875&quot; ## pos neu ## &quot;0&quot; &quot;0.476&quot; ## neg but_count ## &quot;0.524&quot; &quot;0&quot; Each get_vader call generates both the individual scores of each word accounting for its polarity and valence, as well as the four overarching sentiment scores of each string. The first string is strongly positive and is shown to account for both the capitalization of “love” and the heart emoticon. The second string has both a positive connotation through “helpful” but also a negative tone with “not a fan,” leading to a weakly negative compound score. The final string is accurately identified as strongly negative and successfully captures the intention behind the sad face emoticon. Given our validation of VADER’s classification scheme to text very similar to our r/nyc comments, let’s go ahead and create a data frame of the VADER metrics of each of our comments through the vader_df() function. This can take some time to run given our dataset size and your computing resources. I’ve therefore included a “nyc_sentiment.csv” file of these scores pre-generated in the data file of the Github if you’d like to load in the sentiment scores directly instead. 3.2.3 r/nyc Sentiment nyc_sentiment &lt;- vader_df(nyc$body) nyc_sentiment ## Alternatively nyc_sentiment &lt;- read_csv(&#39;nyc_sentiment.csv&#39;) Now that we have generated scores for the entirety of our dataset, let’s investigate what the most high-compound scoring positive and negative posts are respectively. We’ll start on the positive side first through simple dplyr-powered data frame manipulation: top_pos &lt;- nyc_sentiment %&gt;% top_n(5, compound) top_pos$body ## [1] &quot;That’s a really good / interesting question I explain to a lot of people. \\n\\n1.) Rich people have just as many of not more problems than the rest of us,\\n\\n2.) There are quit a few different forms of rich and quite a few ways to get rich. Not all poor people are the same just like not all rich people are the same. I promise you the “higher” your nose is the harder life is and the more you have to “perform” your wealth. Also, there is so many rich people here it’s incredibly diverse in ways I’m s...&quot; ## [2] &quot;Visit Chinatown for some good and cheap eats! But definitely don’t leave NYC without having pizza and bagel =). If you’re willing to travel far one day, eat in Queens - tons of great options! \\n\\nI love cafes and some I really enjoy in Manhattan are Maman, Cha Cha Matcha, La Colombe, 787 cafe, Grace Street, and Round K by Sol. When in the Dumbo area, Arabica is awesome and has a great view of the Brooklyn bridge!&quot; ## [3] &quot;Hi! I agree with this point, you shouldn&#39;t give him away for free because there are a lot of messed up people out there. That being said, I recently adopted a 3.5 year old cat who is a mother that lost her kittens. She is also somewhat shy but loves other cats, so I have been considering a companion for her. I would love to meet your little guy and if it&#39;s a good match I would be happy to pay or partner with an adoption agency for a safer process. Feel free to DM me if you&#39;re interested :)&quot; ## [4] &quot;FINAL UPDATE: I found her (my sister)! But it’s complicated (read past TLDR to find out. If it was an asshole decision, let me know.) Thank you all so so so much! Special thanks to u/lunchboxlou for being the “first responder” if I may. Further gratitude to u/dh1825, u/duckliondog , and u/giveasmile for your amazing support. You all ensured that she would live another day. I absolutely respect all of yours’ commitment.\\n\\nTLDR: I searched for her with my parents. I found her at the station ther...&quot; ## [5] &quot;May be a bit outdated because I moved out of NYC 4 years ago, but here goes. Forest Hills was a pretty big hub for trains so it made it pretty easy to get around. It also had plenty of street parking for my car. Flushing had truly great Asian food. Astoria had some great restaurants and bars, not to mention great Greek food. It also had the cherry blossom festival in Corona Park I think. I liked that Queens was a bit more like the cities I grew up in in Texas due to being able to get around e...&quot; We can see through briefly reviewing the text that the most positive comments under VADER’s classification are speaking highly of particular NYC neighborhoods and restaurants or are expressing positive and friendly sentiment to other users within the subreddit. The first one likely received its high score due to its abudant use of “rich” as a positive term within VADER’s dictionary. Let’s replicate this for the most negative comments. top_neg &lt;- nyc_sentiment %&gt;% top_n(-5, compound) top_neg ## # A tibble: 5 × 6 ## ...1 body compound neg neu pos ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16952 &quot;But they were tested on fetal cells while researching very specific conditions.\\n\\nThe idea that some… -0.992 0.351 0.649 0 ## 2 19203 &quot;Last I checked racists ideas lead to racist actions and racist actions leads to racist being elected … -0.994 0.421 0.539 0.04 ## 3 21477 &quot;seeing criminals being executed in death penalty has an effect to people mind who are about to do hei… -0.996 0.455 0.545 0 ## 4 35037 &quot;NYC&#39;s murder rate is around 5.3 per 100,000\\n\\nFlorida&#39;s murder rate is 5.2\\n\\nTexas murder rate is 4… -0.992 0.383 0.566 0.051 ## 5 41496 &quot;In NY, 66% of prisoners are in for violent crimes\\n\\n\\&quot;Violent crimes\\&quot; is a category that covers eve… -0.992 0.414 0.544 0.042 I’ll skip reading through the details of the most negative comments due to their often disturbing content, but skimming the text lines gives a rather clear picture towards the themes they’re discussing. The fact that these are the most negatively associated comments reflects VADER’s ability to catch the negative emotional valence of text related to violence and discrimination quite effectively. 3.2.4 Sentiment &amp; Score As a final exercise, we’ll explore whether there’s a relationship between an r/NYC post’s community score and its expressed sentiment as identified by VADER. Comments can either be upvoted or downvoted by other users. This produces a score that serves as a proxy for the collective community reaction to a given comment. While most posts receive either no votes or a few upvotes, it can be interesting to see what types of posts lead to outlier cases of a highly positive or negative score in the context of a given subreddit. To prepare for this analysis, I’ll first have to join my separate data frames of both the baseline Reddit data and the VADER scores by comment. I then consider comments with a positive rating score of 20 or above or those that received a net negative score through ggplot2 scatterplots. nyc_full &lt;- merge(nyc, nyc_sentiment, by = &quot;body&quot;) ggplot(nyc_full[which(nyc_full$score&gt;20),], aes(x=compound, y=score)) + geom_point() ggplot(nyc_full[which(nyc_full$score&lt;0),], aes(x=compound, y=score)) + geom_point() It looks like there isn’t a noticable relationship between post score and expressed sentiment for either the most upvoted scores or the most downvoted scores. This alludes to the importance of context and close reading of one’s text data to understand the more abstract themes of what is embraced or contested within the r/nyc subreddit community beyond a simple approach of sentiment analysis through dictionary counts. 3.3 Beyond Dictionaries This concludes our application of a dictionary-based sentiment analysis that can be easily replicated across other datasets to explore emotional polarity and intensity expressed within text. VADER is a particularly helpful tool for social media data, but other great sentiment analyzers avaliable in R are the multiple dictionaries included within tidytext, sentimentr, and SentimentAnalyis. I highly recommend exploring each of the resources further to see how the differences between how they compute sentiment scores and the domains they’re designed to be particularly effective towards may align with your specific research interests. A major limitation of dictionaries is their restricted availability to incorporate words in context to each other. This has promoted alternative sentiment analysis techniques that use supervised machine learning methods to classify expressed sentiment and emotions with a greater attention to context and word co-occurrences. We’ll dig deeper into potential machine learning approaches in the next chapter of this guide by introducing the unsupervised method of topic modeling. "],["ml.html", "Chapter 4 Machine Learning &amp; Topic Models 4.1 Supervised vs. Unsupervised Learning 4.2 Topic Models 4.3 Structural Topic Models", " Chapter 4 Machine Learning &amp; Topic Models Machine learning (ML) has been rapidly increasing in popularity within academia as data science and computational approaches have become more widely known across disciplines. The term can be intimidating for those unfamiliar with its theory and methods. A simplified conceptualization of machine learning is the application of algorithms- a set of instructions grounded in foundational math and statistics- to identify relationships within data to produce informed predictions on. The “learning” in machine learning refers to these method’s ability to perform additional steps within its analysis building from directly programmed instructions via the relationships it is able to learn within your data set. These methods therefore excel at categorization, pattern recognition, and identifying large-scale trends through how each model learns the data it is trained on. 4.1 Supervised vs. Unsupervised Learning Machine learning methods are often differentiated between supervised and unsupervised approaches. Supervised methods are based on labeled data sets where a model learns a relationship to produce a predicted outcome value. “Labeled” data refers to the identified information about the data regarding the features and characteristics of an individual record directly provided to the ML model. Said specified features must be identified ahead of time to use supervised learning methods as the model uses these previously delineated examples to become “trained” in predicting an outcome of interest towards data with similar features. The developed model is then able to replicate its learned task to previously unobserved and often unlabeled data regarding the original factors it’s trained to identify. In contrast, unsupervised approaches let the model guide the learning process through found relationships that were previously unmarked within the data source. Unsupervised methods are particularly well-suited for exploratory research where relationships between factors may not be already known to a given researcher. This is why unsupervised ML methods for NLP are often of distinct interest to social scientists. These tools intrinsically guide emergent findings within data sources and are well suited for large amounts of nuanced text where full-corpus relationships and trends may not be readily known. While machine learning is often associated with Python programming, it’s a lot easier to implement ML models that many social scientists realize within R. There are more and more outstanding R libraries that reduce the learning curve complexity of many ML methods and aggregate a variety of relevant ML packages into a centralized framework. 4.2 Topic Models The method we’ll be highlighting in this demo is topic modeling. Topic models refers to a class of unsupervised ML algorithms that identify common themes within a text corpus by assigning an associated collection of words to a set number of topics. The method is very effective at identifying underlying trends regarding what is commonly discussed within a given text corpus. Topic probabilities are first set to random and then updated as more documents are reviewed by the model. The model processes how words co-occur with each other across the documents of the corpus to create its topical clusters. While topic models are powered by unsupervised machine learning, their discovery of themes within text doesn’t account for the inherent meaning of the identified topics themselves. Interpreting topic model results is a great opportunity for social scientists to employ their domain knowledge to decode how the words assigned to each topic relate to each other. 4.3 Structural Topic Models There are a variety of topic modeling methods with the most commonly employed being Latent Dirichlet Allocation (LDA). We’ll be using an extension of LDA known as Structural Topic Models (STM) to analyze our r/nyc data that was developed by the political scientists and sociologists Molly Roberts, Brandon Stewart, and Dustin Tingley. STM allows for text documents to be associated with multiple identified topics. While I’ll keep the technical details of how STM works brief for the purposes of this introduction, you can think through the model’s underlying techniques as driven by the goal of generating the best topics with their associated words that maximizes the likelihood of having produced the actual observed word trends within your text data. The innovation that STM offers over LDA is linking relevant metadata within your dataset to their variation across topics as well. This essentially incorporates a “covariate” design similar to regression models that are already familiar to quantitative social scientists. We’ll therefore investigate how topic prevalence varies with with a comment’s associated score in our r/nyc sample. 4.3.1 Preprocessing Compared to the dictionary-based sentiment analysis in the previous demo which can handle text data often in its original raw format, topic models are a method that requires extensive pre-processing. Word tokenization, punctuation removal, lowercasing, and stopword removal are all standard steps to prepare data for topic models. For the start of our demo, I first load in our required library and datasets. I then use STM’s built-in document preprocessing function which performs many of the same cleaning and preparation steps featured in the text preprocessing chapter of this guide under the hood. The textProcessor() function’s first argument is the data’s text column, while the metadata parameters refers to any of the remaining covariate you may be interested in exploring their variance with the generated topics. library(readr) library(stm) nyc &lt;- read_csv(&quot;nyc_reddit.csv&quot;) processed &lt;- textProcessor(nyc$body, metadata = nyc, stem = FALSE) Having finished our necessary preprocessing of the r/nyc comments, we then move to reshaping the text into a specific format for topic models known as the document-term matrix. Within this matrix each word token is a row, and each column represents a document within the corpus. We also generate separate objects of the entire vocabulary across our documents as well as for the potential metadata covariate features. All of these components need to be separately prepared to build our topic model’s necessary parameters for analysis. out &lt;- prepDocuments(processed$documents, processed$vocab, processed$meta) docs &lt;- out$documents vocab &lt;- out$vocab meta &lt;- out$meta 4.3.2 Fitting the Model We’re now ready to fit our model. I’ve arbitrarily decided to set our “k” parameter- referring to the number of topics- as 15. This is a comparatively small number for the size of our dataset and the sheer variety of topics that are likely being discussed on r/nyc, but I’ll keep our model small for the sake of our introductory demonstration. I highly recommend Roberts, Stewart, and Tingley’s provided package vignette for STM regarding some features included within the library that can aid with choosing an optimal number of topics with more empirically robust metrics. STM is a complex model that can take some time to run. I’d say it’s generally faster than the VADER analyzer used within the previous guide as a reference for readers who are following along with the chapters in sequence. If you’d like to skip this processing time I included a “nyc_STM.RData” environment in the Github that is available for download. Loading in this environment will automatically have the finished STM results available within your R session, and you’ll be able to skip right to the summary() call of the model results. nyc_redditFit &lt;- stm(documents = out$documents, vocab = out$vocab, K = 15, prevalence =~ score, data = out$meta, init.type = &quot;Spectral&quot;) summary(nyc_redditFit) ## A topic model with 15 topics, 99345 documents and a 30552 word dictionary. ## Topic 1 Top Words: ## Highest Prob: money, pay, will, building, housing, rent, cost ## FREX: rent, units, tenants, tax, nuclear, landlords, tenant ## Lift: carbon, condos, -gw, ‘affordable’, “affordable, “affordable”, “deal” ## Score: money, housing, rent, pay, tax, cost, building ## Topic 2 Top Words: ## Highest Prob: work, job, kids, school, year, home, office ## FREX: remote, unemployment, pension, teachers, ems, teacher, unions ## Lift: -tipping, clerical, cnas, entry-level, gnt, grayson, homeschool ## Score: work, school, kids, job, workers, office, teachers ## Topic 3 Top Words: ## Highest Prob: post, show, read, can, use, article, find ## FREX: card, info, excelsior, link, cards, database, wallet ## Lift: “excelsior, “record, acct, airpods, asknyc, authenticate, authenticated ## Score: card, post, article, read, thanks, thank, app ## Topic 4 Top Words: ## Highest Prob: like, just, one, time, now, good, really ## FREX: yeah, damn, ive, gonna, hope, wish, seen ## Lift: “excuse, actin, backpacks, battlefield, beaters, bedsheets, buisness ## Score: like, just, got, lol, yeah, guy, one ## Topic 5 Top Words: ## Highest Prob: don’t, ’re, think, people, say, just, point ## FREX: racist, downvoted, blm, ’re, slur, don’t, troll ## Lift: “flaw”, “q”, apostrophe, assholes”, backpedal, context”, eloquently ## Score: don’t, ’re, racist, comment, doesn’t, think, can’t ## Topic 6 Top Words: ## Highest Prob: shit, fuck, fucking, little, long, island, big ## FREX: rats, holy, staten, baby, sounds, cool, omg ## Lift: conman, nadlers, perfluorocarbon, raccoon, stroganoff, rats, -ppch ## Score: fuck, shit, island, fucking, staten, sounds, cool ## Topic 7 Top Words: ## Highest Prob: state, government, right, law, case, power, hes ## FREX: constitution, constitutional, amendment, religious, religion, supreme, church ## Lift: amendment, “fascist, “give, “group”, “human, aborted, abortion” ## Score: government, state, law, religious, hes, court, rights ## Topic 8 Top Words: ## Highest Prob: vaccine, covid, vaccinated, people, get, vaccines, getting ## FREX: vaccines, virus, immunity, delta, flu, disease, infection ## Lift: -preparedness, ade, anti-scientific, antigen, antiviral, apcs, arising ## Score: vaccine, vaccinated, covid, vaccines, immunity, virus, unvaccinated ## Topic 9 Top Words: ## Highest Prob: new, day, last, york, thought, first, two ## FREX: bacon, spotted, twin, tower, cheese, bec, empire ## Lift: -acre, -piece, -story, -unit, ‘first-class, “’d, “deeply ## Score: new, york, saw, night, thought, last, day ## Topic 10 Top Words: ## Highest Prob: nyc, city, live, many, people, world, much ## FREX: whites, communities, capita, asian, rural, latino, asians ## Lift: “racial, achievers, actionable, anti-authority, binghamton, brown”, burlington ## Score: nyc, crime, city, cities, population, areas, live ## Topic 11 Top Words: ## Highest Prob: problem, business, restaurant, restaurants, private, etc, order ## FREX: lyft, medallions, medallion, restaurants, customers, dining, cabs ## Lift: bode, brokered, carb, caretaker, creditors, defrauded, exclusivity ## Score: uber, restaurants, business, delivery, private, problem, restaurant ## Topic 12 Top Words: ## Highest Prob: police, cops, talking, nypd, even, yet, guys ## FREX: arrest, arrests, robbery, arrested, rape, cops, attempted ## Lift: advisable, amerigo, arrest, baptists, boogeymen, bowel, chabad ## Score: cops, police, nypd, white, crime, talking, cop ## Topic 13 Top Words: ## Highest Prob: car, cars, street, subway, manhattan, train, park ## FREX: bike, bus, lane, lanes, bikes, buses, pedestrians ## Lift: alleyways, bdfm, bicycles, bidirectional, bikers, bmws, brt ## Score: cars, car, bike, parking, train, manhattan, traffic ## Topic 14 Top Words: ## Highest Prob: water, food, vote, mayor, trump, run, party ## FREX: adams, voting, republican, election, voted, sliwa, voters ## Lift: --city, “food, “vote, aberration, aoc’s, aspirations, avella ## Score: vote, adams, water, mayor, trump, republican, party ## Topic 15 Top Words: ## Highest Prob: people, dont, can, get, want, will, thats ## FREX: dont, cant, want, thats, theyre, happen, wont ## Lift: happend, bac, frightening, omicron, perjury, reintegrated, ticketable ## Score: dont, people, want, thats, youre, cant, get Our 15 generated topics seem to cover a representative range of themes within r/nyc. STM produces a range of associated words with each topic based on different metrics, but we’ll just focus on the first line within each topic that lists the words with the highest probability of being associated with said topic when they occur in a comment. Topic 1 seems to be about local housing, topic 2 about work, topic 3 with words around user engagement in r/nyc related to reading posts and links, topic 4 as informal response words within comment conversation, and so forth. This demonstrates some key takeaways around topic models. First, subjective interpretation is key to understanding what it is the model output means when grouping associated words. Second, some topics demonstrate much more concise associations regarding their underlying subject over others. Let’s deep dive into associated text with topic 15 which has rather conceptually ambiguous associated terms to understand what type of discourse the model is identifying as linked to this topic. The findThoughts() function allows us to retrieve three r/nyc comments that are aligned with topic 15. thoughts15 &lt;- findThoughts(nyc_redditFit, texts = out$meta$body, topics = 15, n=3) thoughts15 ## ## Topic 15: ## Don&#39;t care! Don&#39;t care if they lose their job! Dont care if they&#39;re mad! Dont care about them because they don&#39;t care about other people!!!!!! ## So it&#39;s very telling that in a society and a sub where people will constantly say that if you want something done you should do it yourself.....suddenly that&#39;s not a good idea when this man decides to do it. ## ## Hmmmmm now what could be different.....i can&#39;t QWHITE put my finger on it. ## So basically all the more reason these people should be considered trash? Keep in mind that&#39;s their decision they&#39;re making to put people at risks bc they&#39;re unwilling to fulfill the obligation of the work they have chosen. It&#39;s easy to satisfy your obligations when it&#39;s easy it&#39;s a lot harder when you have to actually do the hard things, like stuff you don&#39;t *want* to do but will bc you have to. These three comments suggests that Topic 15 is picking up discourse around user judgements of appropriate behavior and actions, particularly within argumentative discussions where it appears there are disagreements occurring within the comment conversation thread. This is an example of just how essential close text readings often are within NLP analysis to link model findings to conceptual meanings regarding complex linguistic phenomena. 4.3.3 Variation by Comment Score Let’s do a final investigation within our topic model by allowing topic prevalence to vary by a r/nyc comment’s associated score. Calling estimateEffect() with the “formula” argument specifying which covariate you’d like to vary by topic will achieve this. You can use the summary() command to get the score parameter association with each topic similar to what we obtained with previously summarizing the initial model. I went ahead and highlighted two topics in particular to minimize our net output. score_topics &lt;- estimateEffect(formula = 1:15 ~ score, nyc_redditFit, meta=out$meta, uncertainty=&quot;Global&quot;) summary(score_topics, topics=c(11, 14)) ## ## Call: ## estimateEffect(formula = 1:15 ~ score, stmobj = nyc_redditFit, ## metadata = out$meta, uncertainty = &quot;Global&quot;) ## ## ## Topic 11: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.038e-02 2.463e-04 163.917 &lt;2e-16 *** ## score -4.704e-06 7.987e-06 -0.589 0.556 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Topic 14: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.545e-02 3.267e-04 139.110 &lt; 2e-16 *** ## score 4.934e-05 1.003e-05 4.919 8.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Topic 11’s most probable words includes “problem,” “business,” and “restaurants,” while topic 14 is associated with “water,” “food,” “vote,” and “mayor.” We can see from estimateEffect’s results that topic 11 is significantly associated with comments that receive lower score values, while topic 14 is linked with comments that obtain higher scores. Investigating the associated comments would be a natural next step from these findings to see if additional trends emerge within the text itself that may be relevant to how the comment is reacted to within the wider r/nyc community. 4.3.4 Extending Machine Learning STM is an inherently probabilistic model and will therefore often produce different results depending on your choices of the number of topics and different random number seeds. Topics can be hard to interpret and you’ll want to watch out for issues around multi-meaning “chimera” topics or semantically meaningless topics. Choosing the proper number of topics is rather unsettled process and often significant varies within research applications. Despite these considerations, topic models can be one of the best options for exploratory analysis of text data, particularly for discovering underlying themes and patterns that may not be already known to the researcher. Topic models are just one of many machine learning methods supporting NLP research within R. Additional modeling approaches are a whole separate subtopic beyond this introductory guide with a variety of similar resources that I highly recommend. If there’s a core takeaway I’d like to emphasize with this demo, it’s that ML is a lot more accessible to implement within both R specifically and for previously unfamiliar researchers in general than many realize. "],["methodsethics.html", "Chapter 5 Additional Methods &amp; Ethical Considerations", " Chapter 5 Additional Methods &amp; Ethical Considerations WIP "],["references.html", "Chapter 6 References", " Chapter 6 References WIP "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
