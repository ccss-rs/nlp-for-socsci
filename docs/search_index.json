[["dictionary.html", "Chapter 3 Dictionaries &amp; Sentiment Analysis 3.1 Types of Dictionaries 3.2 Sentiment Analysis 3.3 Beyond Dictionaries", " Chapter 3 Dictionaries &amp; Sentiment Analysis Counting the prevalence and types of words used is a fundamental component of NLP that more complex methods often build from. Word counts offer plentiful information on what it being discussed within a given corpus, how common or rare said words are, who tends to use certain words, and how they differ in their use from other groups of interests. This chapter will provide both an overview of how word counts power the domain of dictionary-based NLP methods as well as walk through an application of counting words that fall into delineated sentiment categories within our r/nyc dataset. Baseline word counts often feature either the entire vocabulary or the counts for all words within a given document. Of particular interest for many social scientists is the prevalence of specific words within text as relevant to an abstract concept or theme. This is where lexical dictionaries shine, referring to any established collection of words used to measure a certain phenomenon that can be applied to various use cases. The measurement of said concept– while scaled and scored in different forms – is often based on initial word counts. Dictionary-based methods are linked in their use of a designated lexicon that claims to accurately capture a variety of words that all measure the concept of interest. Some dictionary methods focus on general lexical features such as pronouns, grammar usage, or asked questions. Other dictionaries attempt to capture more abstract ideas such as identifying words that indicate power, politeness, toxicity, and beyond. For example, a politeness dictionary would have a predetermined list of relevant words such as “please” and “thanks” that can be applied to measure the count of these words within text. Dictionaries require a user to predetermine the relationship between a word and a concept. They can be applied across domains to varying degrees depending on the words that comprise of the dictionary, the rigor in which the dictionary was created, and the specifics of the idea it attempts to measure. 3.1 Types of Dictionaries Dictionaries within text-based social science research are generally divided between those that are built by the researcher for their specific project, and those that were developed by other scholars and then shared for wider use. Building and implementing one’s own dictionary is particularly useful when conducting research relevant to specific communities with unique word usage or when attempting to operationalize concepts with limited precedent within text methods. Robust dictionary design for specialized applications is a unique skill that many social scientists can offer, since we are often domain experts in niche social phenomenon and therefore have particular knowledge towards what words should be included within a new dictionary. However, creating one’s own dictionary also has limitations such as requiring more resources and labor to rigorously produce than sourcing already established dictionaries. The challenges associated with developing one’s own dictionary is a sizable factor behind the popularity of using pre-established dictionaries within research. The creation of a large and conceptually rigorous dictionary is often a project that a whole team has spent considerable time on to identify candidate words from valid sources, cross-check term applicability through relevant agreement metrics, conduct experiments on the dictionary’s validity towards measuring its concept of interest, and finally review its potential applications across other research projects. One of the most famous dictionaries used within social science research is the Linguistic Inquiry Word Count (LIWC). LIWC includes over 70 categories across a wide variety of topics regarding both linguistic features and psychological metrics with a rigorous word validation research process backing each dictionary. Its ability to measure complex phenomenon within text such as emotions, motivational drives, and cognitive processes has driven its use within a wide variety of projects across social science disciplines. A selection of available dictionaries within LIWC LIWC is a great resource that is both rigorously created and well established within the literature. However, it is a proprietary product which limits dictionary access both cost-wise and regarding keeping the individual words within each dictionary private. This can make it challenging to investigate how well said dictionaries will translate over to your specific text data. Part of the beauty of NLP’s interdisciplinary growth is the availability of open-source dictionaries. There are a wide range of options, and I highly recommend conducting reviews on any specific topic you may be interested in exploring within your text data to see what’s out there. Refer to this chapter’s citation list in the References section of the user guide for a selection of open-source dictionaries across a variety of topics. Much like with making your own dictionary, however, you’ll want to bear in mind the rigor and domain applicability of premade dictionaries. It’s often key to review whatever details are available regarding how the dictionary is made to understand its empirical robustness and relevance to your specific use case. This may be provided within resources such as publications that explain the process behind a dictionary’s creation or documentation within a dictionary’s repository or website when accessing its public files. 3.2 Sentiment Analysis Sentiment analysis as applied to product reviews. One specific NLP method that has been adopted across disciplines and is commonly based on established dictionaries is sentiment analysis. Most sentiment classification systems identify individual words as having positive, negative, or neutral connotations and subsequentially generates scores of overall expressed sentiments within text. Sentiment analysis is often used to understand emotions around significant social events, consumer preferences, and political opinions. While a variety of computational methods can be employed to measure sentiment, one of the most common types of sentiment measurement tools are analyzers comprised of multiple dictionaries with words coded as expressing either of the three sentiments. There’s a range of dictionary-based sentiment analyzers that have been developed and tested on various sources such as tweets, online market product reviews, newspaper opinion sections, or movie reviews. This diversity of sentiment tools subsequently can differ in their dictionary terms and how they compute their sentiment scores. Intended sentiment within text is often highly contextual. Sentiment analysis methods therefore also differ in their consideration of intention within word usage. Intention with the negative sentiment of “cheap” referring to a bad product review takes a very different tone when considering a “cheap” deal within an online marketplace. Applying a generic analysis tool in a context where a particular negative or positive phrase is used disproportionately often will likely produce inaccurate results as well. For example, “loss” as often negatively categorized word within a sentiment dictionary would likely misrepresent many posts within a weight loss discussion form. The influence of context is therefore important to additionally consider when choosing a sentiment dictionary for a specific research application. 3.2.1 VADER The Valence Aware Dictionary and Sentiment Reasoner- shortened to VADER- is the dictionary-based sentiment analyzer that we’ll be using in this demo. It was originally written in Python but is available in R through the VADER library. The analyzer’s sentiment dictionaries were created by a team of human raters that scored candidate words for both their emotional polarity (such as positive over negative) and intensity (“amazing” having a higher score than “okay”). VADER generates four scores when applied to text data- positive, negative, neutral, and a compound score that considers all three of these scores to provide an aggregate representation of a text’s overall sentiment. VADER is often the sentiment analyzer of choice for studies that use Reddit data since it is designed to account for common lexical features within social media text. VADER considers more complex influences towards intended sentiment when generating its scores such as negation with “I don’t like” , the underlying sentiment of emoticons and emojis, and the use of emphasis, capitalization, and unique punctuation. It is therefore a particularly well-suited tool for investigating our r/nyc dataset in the following demo. We’ll first load in our relevant dictionaries and dataset. You’ll want to reload the r/nyc data even if you’re following along from the previous text preprocessing chapter to ensure your text data is consistent with the demo as follows. Please install any new packages for your local computer as listed below via install.packages() as well. library(readr) library(dplyr) library(stringr) library(vader) library(ggplot2) nyc &lt;- read_csv(&quot;nyc_reddit.csv&quot;) 3.2.2 String Sentiment Let’s consider examples of individual strings to hone our intuition regarding how VADER generates its sentiment scores. The get_vader() function is our tool of choice to produce individual string scores which I apply to three examples as follows. get_vader(&quot;As someone who always depended on cars before, I LOVE the subway! &lt;3&quot;) ## word_scores ## &quot;{0, 0, 0, 0, 0, 0, 0, 0, 0, 3.933, 0, 0, 1.9}&quot; ## compound ## &quot;0.845&quot; ## pos ## &quot;0.425&quot; ## neu ## &quot;0.575&quot; ## neg ## &quot;0&quot; ## but_count ## &quot;0&quot; get_vader(&quot;The subway is very helpful, but I&#39;m not a fan of the rats.&quot;) ## word_scores ## &quot;{0, 0, 0, 0, 1.0465, 0, 0, 0, 0, -1.443, 0, 0, 0}&quot; ## compound ## &quot;-0.102&quot; ## pos ## &quot;0.132&quot; ## neu ## &quot;0.71&quot; ## neg ## &quot;0.158&quot; ## but_count ## &quot;1&quot; get_vader(&quot;I hate how delayed the subway always is… being late for work sucks. :(&quot;) ## word_scores ## &quot;{0, -2.7, 0, -0.9, 0, 0, 0, 0, 0, 0, 0, 0, -1.5, -1.9}&quot; ## compound ## &quot;-0.875&quot; ## pos ## &quot;0&quot; ## neu ## &quot;0.476&quot; ## neg ## &quot;0.524&quot; ## but_count ## &quot;0&quot; Each get_vader call generates both the individual scores of each word accounting for its polarity and valence, as well as the four overarching sentiment scores of each string. The first string is strongly positive and is shown to account for both the capitalization of “love” and the heart emoji. The second string has both a positive connotation through “helpful” but also a negative tone with “not a fan,” leading to a weakly negative compound score. The final string is accurately identified as strongly negative and successfully captures the intention behind the sad face emoticon. Given our validation of VADER’s classification scheme to text very similar to our r/nyc comments, let’s go ahead and create a data frame of the VADER metrics of each of our comments through the vader_df() function. This can take some time to run given our dataset size and your computing resources. I’ve therefore included an “nyc_sentiment.csv” file of these scores pre-generated in the data file of the Github if you’d like to load in the sentiment scores directly instead. 3.2.3 r/nyc Sentiment nyc_sentiment &lt;- vader_df(nyc$body) nyc_sentiment ## Alternatively nyc_sentiment &lt;- read_csv(&#39;nyc_sentiment.csv&#39;) Now that we have generated scores for the entirety of our dataset, let’s investigate what the most high-compound scoring positive and negative posts are respectively. We’ll start on the positive side first through simple dplyr-powered data frame manipulation: top_pos &lt;- nyc_sentiment %&gt;% top_n(5, compound) top_pos$body ## [1] &quot;That’s a really good / interesting question I explain to a lot of people. \\n\\n1.) Rich people have just as many of not more problems than the rest of us,\\n\\n2.) There are quit a few different forms of rich and quite a few ways to get rich. Not all poor people are the same just like not all rich people are the same. I promise you the “higher” your nose is the harder life is and the more you have to “perform” your wealth. Also, there is so many rich people here it’s incredibly diverse in ways I’m s...&quot; ## [2] &quot;Visit Chinatown for some good and cheap eats! But definitely don’t leave NYC without having pizza and bagel =). If you’re willing to travel far one day, eat in Queens - tons of great options! \\n\\nI love cafes and some I really enjoy in Manhattan are Maman, Cha Cha Matcha, La Colombe, 787 cafe, Grace Street, and Round K by Sol. When in the Dumbo area, Arabica is awesome and has a great view of the Brooklyn bridge!&quot; ## [3] &quot;Hi! I agree with this point, you shouldn&#39;t give him away for free because there are a lot of messed up people out there. That being said, I recently adopted a 3.5 year old cat who is a mother that lost her kittens. She is also somewhat shy but loves other cats, so I have been considering a companion for her. I would love to meet your little guy and if it&#39;s a good match I would be happy to pay or partner with an adoption agency for a safer process. Feel free to DM me if you&#39;re interested :)&quot; ## [4] &quot;FINAL UPDATE: I found her (my sister)! But it’s complicated (read past TLDR to find out. If it was an asshole decision, let me know.) Thank you all so so so much! Special thanks to u/lunchboxlou for being the “first responder” if I may. Further gratitude to u/dh1825, u/duckliondog , and u/giveasmile for your amazing support. You all ensured that she would live another day. I absolutely respect all of yours’ commitment.\\n\\nTLDR: I searched for her with my parents. I found her at the station ther...&quot; ## [5] &quot;May be a bit outdated because I moved out of NYC 4 years ago, but here goes. Forest Hills was a pretty big hub for trains so it made it pretty easy to get around. It also had plenty of street parking for my car. Flushing had truly great Asian food. Astoria had some great restaurants and bars, not to mention great Greek food. It also had the cherry blossom festival in Corona Park I think. I liked that Queens was a bit more like the cities I grew up in in Texas due to being able to get around e...&quot; We can see through briefly reviewing the text that the most positive comments under VADER’s classification are speaking highly of particular NYC neighborhoods and restaurants or are expressing positive and friendly sentiment to other users within the subreddit. The first one likely received its high score due to its abudant use of “rich” as a positive term within VADER’s dictionary. Let’s replicate this for the most negative comments. top_neg &lt;- nyc_sentiment %&gt;% top_n(-5, compound) top_neg ## # A tibble: 5 × 6 ## ...1 body compound neg neu pos ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16952 &quot;But they were tested on fetal cells while r… -0.992 0.351 0.649 0 ## 2 19203 &quot;Last I checked racists ideas lead to racist… -0.994 0.421 0.539 0.04 ## 3 21477 &quot;seeing criminals being executed in death pe… -0.996 0.455 0.545 0 ## 4 35037 &quot;NYC&#39;s murder rate is around 5.3 per 100,000… -0.992 0.383 0.566 0.051 ## 5 41496 &quot;In NY, 66% of prisoners are in for violent … -0.992 0.414 0.544 0.042 I’ll skip reading through the details of the most negative comments due to their often disturbing content, but skimming the text lines gives a rather clear picture towards the themes they’re discussing. The fact that these are the most negatively associated comments reflects VADER’s ability to catch the negative emotional valence of text related to violence and discrimination quite effectively. 3.2.4 Sentiment &amp; Score As a final exercise, I’m interested in exploring if there’s a relationship between an r/NYC post’s community score and its expressed sentiment as identified by VADER. Comments can either be upvoted or downvoted by other users. This produces a score that serves as a proxy for the collective community reaction to a given comment. While most posts receive either no votes or a few upvotes, it can be interesting to see what types of posts lead to outlier cases of a highly positive or negative score in the context of a given subreddit. To prepare for this analysis, I’ll first have to join my separate data frames of both the baseline Reddit data and the VADER scores by comment I then consider comments with a positive rating score of 20 or above or those that received a net negative score through ggplot2 scatterplots. nyc_full &lt;- merge(nyc, nyc_sentiment, by = &quot;body&quot;) ggplot(nyc_full[which(nyc_full$score&gt;20),], aes(x=compound, y=score)) + geom_point() ggplot(nyc_full[which(nyc_full$score&lt;0),], aes(x=compound, y=score)) + geom_point() It looks like there isn’t a noticable relationship between post score and expressed sentiment for either the most upvoted scores or the most downvoted scores. This alludes to the importance of context and close reading of one’s text data to understand the more abstract themes of what is embraced or contested within the r/nyc subreddit community beyond a simple approach of sentiment analysis through dictionary counts. 3.3 Beyond Dictionaries This concludes our simple application of a dictionary-based sentiment analysis that can be easily replicated across other datasets to explore emotional polarity and intensity expressed within text. VADER is a particularly helpful tool for social media data, but other great sentiment analyzers within R include the multiple dictionaries included within tidytext, sentimentr, and SentimentAnalyis. I highly recommend exploring each of the resources further to see how the differences between how they compute sentiment scores and the domains they’re designed to be particularly effective towards may align with your specific research interests. A major limitation of dictionaries is their restricted availability to incorporate words in context to each other. This has promoted an alternative sentiment analysis technique to use supervised machine learning methods to classify expressed sentiment and emotions with a greater attention to context and word co-occurrences. We’ll dig deeper into potential machine learning approaches in the next chapter of this guide by introducing the unsupervised method of topic modeling. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
