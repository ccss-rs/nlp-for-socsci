<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Summarise Tables</title>
  <meta name="description" content="Learn core commands for text pre-processing." />
  <meta name="generator" content="bookdown 0.24.3 and GitBook 2.6.7" />

  <meta property="og:title" content="Summarise Tables" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Learn core commands for text pre-processing." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Summarise Tables" />
  
  <meta name="twitter:description" content="Learn core commands for text pre-processing." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a>Natural Language Prcoessing for the Social Sciences</a></li>

<li class="divider"></li>
<li class="chapter" data-level="0.1" data-path=""><a href="#section-donwload-load-the-pos-tags-for-english"><i class="fa fa-check"></i><b>0.1</b> Donwload &amp; load the POS tags for English</a></li>
<li class="chapter" data-level="0.2" data-path=""><a href="#section-annotation-will-take-quite-some-time-particularly-if-youre-using-the"><i class="fa fa-check"></i><b>0.2</b> Annotation will take quite some time, particularly if you’re using the</a></li>
<li class="chapter" data-level="0.3" data-path=""><a href="#section-non-stopword-version-of-our-tokenized-dataset"><i class="fa fa-check"></i><b>0.3</b> non-stopword version of our tokenized dataset</a></li>
<li class="chapter" data-level="0.4" data-path=""><a href="#section-the-generated-upos-column-in-the-following-dataframe-will-have-each-tokens"><i class="fa fa-check"></i><b>0.4</b> The generated “upos” column in the following dataframe will have each token’s</a></li>
<li class="chapter" data-level="0.5" data-path=""><a href="#section-identified-pos-tags."><i class="fa fa-check"></i><b>0.5</b> identified POS tags.</a></li>
<li class="divider"></li>
<li><a>Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Summarise Tables</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-header">
<h1 class="title">Summarise Tables</h1>
</div>
<p>```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(stringr)
library(tidytext)
library(SnowballC)
library(learnr)
tutorial.options(exercise.completion = FALSE)</p>
<p>nyc &lt;- read_csv(“nyc_10k.csv”)</p>
<pre><code>
In this demo we&#39;ll be walking through how to perform the fundamentals of text 
pre-processing within R. Let&#39;s briefly go over the core packages that I&#39;ve pre-downloaded 
into our session:
* **readr** facilitates the easy uploading of external data files such as CSVs
* **dplyr** promotes streamlined data frame creation and manipulation
* **stringr** is all about working with text character strings
* **tidytext** provides more specialized functions to prepare text data for various NLP use cases
* **SnowballC**

These libraries are all popular choices for performing essential text pre-processing
functions. I&#39;ll also load in our working data set of 10,000 comments scraped from
r/nyc.

```{r, eval=FALSE}
library(readr)
library(dplyr)
library(stringr)
library(tidytext)
library(SnowballC)

nyc &lt;- read_csv(&quot;nyc_10k.csv&quot;)</code></pre>
<p>Let’s check the character encoding of our r/nyc text data. The “body” column of the
loaded data frame features the text of the scraped r/nyc comments.</p>
<p><code>{r encoding, exercise = TRUE, exercise.setup="setup"} guess_encoding(nyc$body)</code></p>
<p>Our 1 confidence score on the UTF-8 encoding for the text column of our data frame
assures us that the following pre-processing methods should work without
problems around character encoding. Let’s now inspect our data set in more detail.</p>
<p><code>{r data} nyc</code></p>
<p>This provides us with the first 10 records of our data set. “id” is the unique
identifier for each comment, “author” provides the Reddit username who wrote the
comment, “body” features the comments themselves, “score” refers to the
Reddit system of being able to “upvote” or “downvote” comments similar to a
like/dislike framework, and “date” as the year, month, and day the comment was
posted.</p>
<p>You’ll notice with a preliminary view on the comments that there’s unwanted noise
within the text, such as ‘<code>\n</code>’ referring to line breaks written within HTML. Let’s
use a stringr function to remove all instances of ‘<code>\n</code>’ from the comment text.</p>
<p><code>{r remove, exercise = TRUE, exercise.setup="setup"} nyc$body &lt;- str_remove_all(nyc$body, "\n")</code>
This successfully removes all instances of “<code>\n</code>” within the comments. There’s a
wide range of additional types of noise a researcher may want to remove from text.
While our last str_remove_all command is quite effective at removing exact matches
with ‘<code>\n</code>’, regexes are a particularly helpful alternative for matching a wider range
of character types to delete within one function call.</p>
<p>One common use case for regexes within text pre-processing is to remove all types
of punctuation, since many NLP methods would otherwise consider tokenized punctuation
to be there own words. Luckily, there’s special regex character classes that specialize
in matching with groups of characters. Let’s use one to identify and remove all
punctuation from our text as follows.</p>
<p><code>{r regex, exercise = TRUE, exercise.setup="remove"} nyc$body &lt;- str_remove_all(nyc$body, '[:punct:]')</code></p>
<p>It’s common within text pre-processing to convert all words to lowercase since
capitalization may lead an NLP method to consider the same words as separate
entities based on casing alone. You may also consider combining n-grams into
single word units, such as concatenating all instances of the bi-gram “New York”
into the single token of “New_York.” Let’s learn two more stringr functions to
accomplish both of these goals within our comment data.</p>
<p><code>{r lowerreplace, exercise = TRUE, exercise.setup="regex"} nyc$body &lt;- str_to_lower(nyc$body) nyc$body &lt;- str_replace_all(nyc$body, "New York", "New_York")</code></p>
<p><em>Tokenization</em> refers to the segmentation of text components into unit such as
individual words, sentences, paragraphs, characters, or n-grams. Let’s use
the tidytext package to create a new data frame of our r/nyc comments split
into word tokens.</p>
<p><code>{r unnest, exercise = TRUE, exercise.setup="lowerreplace"} tokens &lt;- nyc %&gt;%     unnest_tokens("word", body)</code>
Our 100,000 original comments are now almost 3.5 million rows of individual word
tokens. By default, unnest_tokens lowercases the tokens and removes punctuation
from the original string, which would allow you to skip our previous regex string
removal step.</p>
<p>Let’s now consider the optional pre-processing steps we covered in the slides.
First is <em>stopword removal</em>, which refers to common words such as “and”, “the”,
“but”, and so forth that we’d like to remove from our text data since we consider
them semantically irrelevant to our core NLP interests. Tidytext comes with a
pre-made common stopwords list that we can call and apply directly to our generated
tokens data frame.</p>
<p>```{r stopwords, exercise = TRUE, exercise.setup=“unnest”}
data(stop_words)</p>
<p>tokens &lt;- tokens %&gt;%
anti_join(stop_words)</p>
<pre><code>
*Stemming* is used to manipulate text into its root word form, such as &quot;swimmers&quot;
and &quot;swimming&quot; being stemmed to &quot;swim&quot;. There’s a variety of different stemming 
algorithms available with the most commonly used being the Porter stemmer. We can
use the methods from the SnowballC package to stem our own tokenized r/nyc comments 
as follows. 

```{r stemming, exercise = TRUE, exercise.setup=&quot;unnest&quot;}
stems &lt;- tokens %&gt;%
  mutate(stem = wordStem(word)) %&gt;%
  count(stem, sort = TRUE)</code></pre>
<p>Finally, parts-of-speech (POS) tags identifies the grammatic and lexical classes
that words in your text data belong to. There’s a variety of different libraries
that follow different POS tag rules, with the “udpipe”- referring to the Universal
Dependencies tagging schema- being a popular library choice for R.</p>
<p>We won’t run this code chunk within the demo itself since it usually takes some
time to process, but feel free to experiment with POS tags on your own time
following the below work flow.</p>
<p>```{r partsofspeech, eval=FALSE}
library(“udpipe”)</p>
<div id="section-donwload-load-the-pos-tags-for-english" class="section level2" number="0.1">
<h2><span class="header-section-number">0.1</span> Donwload &amp; load the POS tags for English</h2>
<p>model &lt;- udpipe_download_model(language = “english”)
model &lt;- udpipe_load_model(model)</p>
</div>
<div id="section-annotation-will-take-quite-some-time-particularly-if-youre-using-the" class="section level2" number="0.2">
<h2><span class="header-section-number">0.2</span> Annotation will take quite some time, particularly if you’re using the</h2>
</div>
<div id="section-non-stopword-version-of-our-tokenized-dataset" class="section level2" number="0.3">
<h2><span class="header-section-number">0.3</span> non-stopword version of our tokenized dataset</h2>
<p>tags &lt;- udpipe_annotate(model, x = tokens$word)</p>
</div>
<div id="section-the-generated-upos-column-in-the-following-dataframe-will-have-each-tokens" class="section level2" number="0.4">
<h2><span class="header-section-number">0.4</span> The generated “upos” column in the following dataframe will have each token’s</h2>
</div>
<div id="section-identified-pos-tags." class="section level2" number="0.5">
<h2><span class="header-section-number">0.5</span> identified POS tags.</h2>
<p>pos &lt;- as.data.frame(tags)
```</p>
You’ve now completed the pre-processing demo and established a working knowledge
of how to perform the fundamentals of text data pre-processing in R. Congrats!
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{},"value":[{"type":"NULL"}]}
</script>
<!--/html_preserve-->
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/%s",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": false,
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
